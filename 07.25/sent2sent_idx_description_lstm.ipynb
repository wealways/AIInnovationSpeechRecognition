{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"A barber is a person. \n",
    "a barber is good person. \n",
    "a barber is huge person. \n",
    "he Knew A Secret! \n",
    "The Secret He Kept is huge secret. \n",
    "Huge secret. \n",
    "His barber kept his word. \n",
    "a barber kept his word. \n",
    "His barber kept his secret. \n",
    "But keeping and keeping such a huge secret to himself was driving the barber crazy. \n",
    "the barber went up a huge mountain.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 목표 1: Word Seq --> Word-ID Seq\n",
    "\n",
    "## 세부목표 1: 문장을 토큰화\n",
    "## 세부목표 2: 단어 Set 및 아이디 부여\n",
    "## 세부목표 3: Word Seq --> Word-ID Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A barber is a person . \n",
      "a barber is good person . \n",
      "a barber is huge person . \n",
      "he Knew A Secret ! \n",
      "The Secret He Kept is huge secret . \n",
      "Huge secret . \n",
      "His barber kept his word . \n",
      "a barber kept his word . \n",
      "His barber kept his secret . \n",
      "But keeping and keeping such a huge secret to himself was driving the barber crazy . \n",
      "the barber went up a huge mountain .\n"
     ]
    }
   ],
   "source": [
    "print(text.replace(\".\", \" .\").replace(\"!\", \" !\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A barber is a person . ', 'a barber is good person . ', 'a barber is huge person . ', 'he Knew A Secret! ', 'The Secret He Kept is huge secret . ', 'Huge secret . ', 'His barber kept his word . ', 'a barber kept his word . ', 'His barber kept his secret . ', 'But keeping and keeping such a huge secret to himself was driving the barber crazy . ', 'the barber went up a huge mountain .']\n"
     ]
    }
   ],
   "source": [
    "token = text.replace(\".\", \" .\").split('\\n')\n",
    "print(token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ = text.replace(\".\", \" .\").replace(\"!\", \" !\")\n",
    "text_ = text_.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sent = text_.lower().split('\\n')\n",
    "# print(text_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a', 'barber', 'is', 'a', 'person', '.'], ['a', 'barber', 'is', 'good', 'person', '.'], ['a', 'barber', 'is', 'huge', 'person', '.'], ['he', 'knew', 'a', 'secret', '!'], ['the', 'secret', 'he', 'kept', 'is', 'huge', 'secret', '.'], ['huge', 'secret', '.'], ['his', 'barber', 'kept', 'his', 'word', '.'], ['a', 'barber', 'kept', 'his', 'word', '.'], ['his', 'barber', 'kept', 'his', 'secret', '.'], ['but', 'keeping', 'and', 'keeping', 'such', 'a', 'huge', 'secret', 'to', 'himself', 'was', 'driving', 'the', 'barber', 'crazy', '.'], ['the', 'barber', 'went', 'up', 'a', 'huge', 'mountain', '.']]\n"
     ]
    }
   ],
   "source": [
    "sent = []\n",
    "\n",
    "for s in text_sent:\n",
    "    word_seq = s.split()\n",
    "    sent.append(word_seq)\n",
    "    # print(word_seq)\n",
    "\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 8, 'barber': 8, 'is': 4, 'person': 3, '.': 10, 'good': 1, 'huge': 5, 'he': 2, 'knew': 1, 'secret': 6, '!': 1, 'the': 3, 'kept': 4, 'his': 5, 'word': 2, 'but': 1, 'keeping': 2, 'and': 1, 'such': 1, 'to': 1, 'himself': 1, 'was': 1, 'driving': 1, 'crazy': 1, 'went': 1, 'up': 1, 'mountain': 1}\n"
     ]
    }
   ],
   "source": [
    "count_dict = {}\n",
    "\n",
    "for s in sent:\n",
    "    for w in s:\n",
    "        if w not in count_dict:\n",
    "            count_dict[w] = 1\n",
    "        else:\n",
    "            count_dict[w] += 1\n",
    "\n",
    "print(count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('a', 8), ('barber', 8), ('is', 4), ('person', 3), ('.', 10), ('good', 1), ('huge', 5), ('he', 2), ('knew', 1), ('secret', 6), ('!', 1), ('the', 3), ('kept', 4), ('his', 5), ('word', 2), ('but', 1), ('keeping', 2), ('and', 1), ('such', 1), ('to', 1), ('himself', 1), ('was', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('up', 1), ('mountain', 1)])\n"
     ]
    }
   ],
   "source": [
    "print(count_dict.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocab_sorted=sorted(count_dict.items(), key=lambda x:x[1], reverse=True)\n",
    "print(vocab_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 10), ('a', 8), ('barber', 8), ('secret', 6), ('huge', 5), ('his', 5), ('is', 4), ('kept', 4), ('person', 3), ('the', 3), ('he', 2), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('!', 1), ('but', 1), ('and', 1), ('such', 1), ('to', 1), ('himself', 1), ('was', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('up', 1), ('mountain', 1)]\n"
     ]
    }
   ],
   "source": [
    "vocab_sorted=sorted(count_dict.items(), key=lambda x:x[1], reverse=True)\n",
    "print(vocab_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 1, 'a': 2, 'barber': 3, 'secret': 4, 'huge': 5, 'his': 6, 'is': 7, 'kept': 8, 'person': 9, 'the': 10, 'he': 11, 'word': 12, 'keeping': 13, 'good': 14, 'knew': 15, '!': 16, 'but': 17, 'and': 18, 'such': 19, 'to': 20, 'himself': 21, 'was': 22, 'driving': 23, 'crazy': 24, 'went': 25, 'up': 26, 'mountain': 27}\n"
     ]
    }
   ],
   "source": [
    "vocab_dict = {}\n",
    "\n",
    "for idx, (word, word_count) in enumerate(vocab_sorted):\n",
    "    # print(word, word_count)\n",
    "    vocab_dict[word] = idx+1\n",
    "\n",
    "print(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "a = [0]*27\n",
    "a[12] = 1\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "b = [0]*27\n",
    "b[vocab_dict['keeping']] = 1\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_id : a 2 [2]\n",
      "s_id : barber 3 [2, 3]\n",
      "s_id : is 7 [2, 3, 7]\n",
      "s_id : a 2 [2, 3, 7, 2]\n",
      "s_id : person 9 [2, 3, 7, 2, 9]\n",
      "s_id : . 1 [2, 3, 7, 2, 9, 1]\n",
      "\n",
      "s_id : a 2 [2]\n",
      "s_id : barber 3 [2, 3]\n",
      "s_id : is 7 [2, 3, 7]\n",
      "s_id : good 14 [2, 3, 7, 14]\n",
      "s_id : person 9 [2, 3, 7, 14, 9]\n",
      "s_id : . 1 [2, 3, 7, 14, 9, 1]\n",
      "\n",
      "s_id : a 2 [2]\n",
      "s_id : barber 3 [2, 3]\n",
      "s_id : is 7 [2, 3, 7]\n",
      "s_id : huge 5 [2, 3, 7, 5]\n",
      "s_id : person 9 [2, 3, 7, 5, 9]\n",
      "s_id : . 1 [2, 3, 7, 5, 9, 1]\n",
      "\n",
      "s_id : he 11 [11]\n",
      "s_id : knew 15 [11, 15]\n",
      "s_id : a 2 [11, 15, 2]\n",
      "s_id : secret 4 [11, 15, 2, 4]\n",
      "s_id : ! 16 [11, 15, 2, 4, 16]\n",
      "\n",
      "s_id : the 10 [10]\n",
      "s_id : secret 4 [10, 4]\n",
      "s_id : he 11 [10, 4, 11]\n",
      "s_id : kept 8 [10, 4, 11, 8]\n",
      "s_id : is 7 [10, 4, 11, 8, 7]\n",
      "s_id : huge 5 [10, 4, 11, 8, 7, 5]\n",
      "s_id : secret 4 [10, 4, 11, 8, 7, 5, 4]\n",
      "s_id : . 1 [10, 4, 11, 8, 7, 5, 4, 1]\n",
      "\n",
      "s_id : huge 5 [5]\n",
      "s_id : secret 4 [5, 4]\n",
      "s_id : . 1 [5, 4, 1]\n",
      "\n",
      "s_id : his 6 [6]\n",
      "s_id : barber 3 [6, 3]\n",
      "s_id : kept 8 [6, 3, 8]\n",
      "s_id : his 6 [6, 3, 8, 6]\n",
      "s_id : word 12 [6, 3, 8, 6, 12]\n",
      "s_id : . 1 [6, 3, 8, 6, 12, 1]\n",
      "\n",
      "s_id : a 2 [2]\n",
      "s_id : barber 3 [2, 3]\n",
      "s_id : kept 8 [2, 3, 8]\n",
      "s_id : his 6 [2, 3, 8, 6]\n",
      "s_id : word 12 [2, 3, 8, 6, 12]\n",
      "s_id : . 1 [2, 3, 8, 6, 12, 1]\n",
      "\n",
      "s_id : his 6 [6]\n",
      "s_id : barber 3 [6, 3]\n",
      "s_id : kept 8 [6, 3, 8]\n",
      "s_id : his 6 [6, 3, 8, 6]\n",
      "s_id : secret 4 [6, 3, 8, 6, 4]\n",
      "s_id : . 1 [6, 3, 8, 6, 4, 1]\n",
      "\n",
      "s_id : but 17 [17]\n",
      "s_id : keeping 13 [17, 13]\n",
      "s_id : and 18 [17, 13, 18]\n",
      "s_id : keeping 13 [17, 13, 18, 13]\n",
      "s_id : such 19 [17, 13, 18, 13, 19]\n",
      "s_id : a 2 [17, 13, 18, 13, 19, 2]\n",
      "s_id : huge 5 [17, 13, 18, 13, 19, 2, 5]\n",
      "s_id : secret 4 [17, 13, 18, 13, 19, 2, 5, 4]\n",
      "s_id : to 20 [17, 13, 18, 13, 19, 2, 5, 4, 20]\n",
      "s_id : himself 21 [17, 13, 18, 13, 19, 2, 5, 4, 20, 21]\n",
      "s_id : was 22 [17, 13, 18, 13, 19, 2, 5, 4, 20, 21, 22]\n",
      "s_id : driving 23 [17, 13, 18, 13, 19, 2, 5, 4, 20, 21, 22, 23]\n",
      "s_id : the 10 [17, 13, 18, 13, 19, 2, 5, 4, 20, 21, 22, 23, 10]\n",
      "s_id : barber 3 [17, 13, 18, 13, 19, 2, 5, 4, 20, 21, 22, 23, 10, 3]\n",
      "s_id : crazy 24 [17, 13, 18, 13, 19, 2, 5, 4, 20, 21, 22, 23, 10, 3, 24]\n",
      "s_id : . 1 [17, 13, 18, 13, 19, 2, 5, 4, 20, 21, 22, 23, 10, 3, 24, 1]\n",
      "\n",
      "s_id : the 10 [10]\n",
      "s_id : barber 3 [10, 3]\n",
      "s_id : went 25 [10, 3, 25]\n",
      "s_id : up 26 [10, 3, 25, 26]\n",
      "s_id : a 2 [10, 3, 25, 26, 2]\n",
      "s_id : huge 5 [10, 3, 25, 26, 2, 5]\n",
      "s_id : mountain 27 [10, 3, 25, 26, 2, 5, 27]\n",
      "s_id : . 1 [10, 3, 25, 26, 2, 5, 27, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sent_idx = []\n",
    "\n",
    "for s in sent:\n",
    "    s_id = []\n",
    "    for w in s:\n",
    "        w_id = vocab_dict[w]\n",
    "        s_id.append(w_id)\n",
    "        print(\"s_id :\", w, w_id, s_id)\n",
    "    sent_idx.append(s_id)\n",
    "    print()\n",
    "\n",
    "# print(sent_idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'barber', 'is', 'a', 'person', '.']\n",
      "[2, 3, 7, 2, 9, 1]\n",
      "\n",
      "['a', 'barber', 'is', 'good', 'person', '.']\n",
      "[2, 3, 7, 14, 9, 1]\n",
      "\n",
      "['a', 'barber', 'is', 'huge', 'person', '.']\n",
      "[2, 3, 7, 5, 9, 1]\n",
      "\n",
      "['he', 'knew', 'a', 'secret', '!']\n",
      "[11, 15, 2, 4, 16]\n",
      "\n",
      "['the', 'secret', 'he', 'kept', 'is', 'huge', 'secret', '.']\n",
      "[10, 4, 11, 8, 7, 5, 4, 1]\n",
      "\n",
      "['huge', 'secret', '.']\n",
      "[5, 4, 1]\n",
      "\n",
      "['his', 'barber', 'kept', 'his', 'word', '.']\n",
      "[6, 3, 8, 6, 12, 1]\n",
      "\n",
      "['a', 'barber', 'kept', 'his', 'word', '.']\n",
      "[2, 3, 8, 6, 12, 1]\n",
      "\n",
      "['his', 'barber', 'kept', 'his', 'secret', '.']\n",
      "[6, 3, 8, 6, 4, 1]\n",
      "\n",
      "['but', 'keeping', 'and', 'keeping', 'such', 'a', 'huge', 'secret', 'to', 'himself', 'was', 'driving', 'the', 'barber', 'crazy', '.']\n",
      "[17, 13, 18, 13, 19, 2, 5, 4, 20, 21, 22, 23, 10, 3, 24, 1]\n",
      "\n",
      "['the', 'barber', 'went', 'up', 'a', 'huge', 'mountain', '.']\n",
      "[10, 3, 25, 26, 2, 5, 27, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s, s_idx in zip(sent, sent_idx):\n",
    "    print(s)\n",
    "    print(s_idx)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 6, 6, 5, 8, 3, 6, 6, 6, 16, 8]\n"
     ]
    }
   ],
   "source": [
    "lengths = []\n",
    "for s in sent_idx:\n",
    "    len_sent = len(s)\n",
    "    lengths.append(len_sent)\n",
    "    \n",
    "print(lengths)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "max_length = max(lengths)\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "max_length = 0\n",
    "for s in sent_idx:\n",
    "    len_sent = len(s)\n",
    "    if len_sent > max_length:\n",
    "        max_length = len_sent\n",
    "    \n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3, 7, 2, 9, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 3, 7, 14, 9, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 3, 7, 5, 9, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [11, 15, 2, 4, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [10, 4, 11, 8, 7, 5, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0], [5, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [6, 3, 8, 6, 12, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 3, 8, 6, 12, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [6, 3, 8, 6, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [17, 13, 18, 13, 19, 2, 5, 4, 20, 21, 22, 23, 10, 3, 24, 1], [10, 3, 25, 26, 2, 5, 27, 1, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "sent_idx_zeropadded = []\n",
    "\n",
    "for s in sent_idx:\n",
    "    # print(s)\n",
    "    temp = [0]*max_length\n",
    "    for idx, w_id in enumerate(s):\n",
    "        temp[idx] = s[idx]\n",
    "    # print(temp)\n",
    "    sent_idx_zeropadded.append(temp)\n",
    "    # break\n",
    "\n",
    "print(sent_idx_zeropadded)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data_X = np.asarray(sent_idx_zeropadded, dtype=np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 16)\n",
      "<class 'numpy.ndarray'>\n",
      "[2. 3. 7. 2. 9. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(data_X.shape)\n",
    "print(type(data_X[0]))\n",
    "print(data_X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.  3.  7.  2.  9.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 2.  3.  7. 14.  9.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 2.  3.  7.  5.  9.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [11. 15.  2.  4. 16.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [10.  4. 11.  8.  7.  5.  4.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 5.  4.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 6.  3.  8.  6. 12.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 2.  3.  8.  6. 12.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 6.  3.  8.  6.  4.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [17. 13. 18. 13. 19.  2.  5.  4. 20. 21. 22. 23. 10.  3. 24.  1.]\n",
      " [10.  3. 25. 26.  2.  5. 27.  1.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "[[ 3.  7.  2.  9.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 3.  7. 14.  9.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 3.  7.  5.  9.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [15.  2.  4. 16.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 4. 11.  8.  7.  5.  4.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 4.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 3.  8.  6. 12.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 3.  8.  6. 12.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 3.  8.  6.  4.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [13. 18. 13. 19.  2.  5.  4. 20. 21. 22. 23. 10.  3. 24.  1.  0.]\n",
      " [ 3. 25. 26.  2.  5. 27.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(data_X)\n",
    "data_Y= np.roll(data_X, shift=-1, axis=1)\n",
    "for row in range(0, len(data_Y)):\n",
    "    # print(row)\n",
    "    data_Y[row, len(data_Y[row])-1] = 0 \n",
    "print(data_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 1, 'a': 2, 'barber': 3, 'secret': 4, 'huge': 5, 'his': 6, 'is': 7, 'kept': 8, 'person': 9, 'the': 10, 'he': 11, 'word': 12, 'keeping': 13, 'good': 14, 'knew': 15, '!': 16, 'but': 17, 'and': 18, 'such': 19, 'to': 20, 'himself': 21, 'was': 22, 'driving': 23, 'crazy': 24, 'went': 25, 'up': 26, 'mountain': 27}\n",
      "28\n",
      "[[ 2.  3.  7.  2.  9.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 2.  3.  7. 14.  9.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 2.  3.  7.  5.  9.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [11. 15.  2.  4. 16.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [10.  4. 11.  8.  7.  5.  4.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 5.  4.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 6.  3.  8.  6. 12.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 2.  3.  8.  6. 12.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 6.  3.  8.  6.  4.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [17. 13. 18. 13. 19.  2.  5.  4. 20. 21. 22. 23. 10.  3. 24.  1.]\n",
      " [10.  3. 25. 26.  2.  5. 27.  1.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "print(vocab_dict)\n",
    "num_classes = len(vocab_dict) + 1\n",
    "print(num_classes)\n",
    "print(data_X)\n",
    "data_XX = to_categorical(data_X, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 1. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 1. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 1. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 1. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "print(data_XX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_XX = to_categorical(data_X, num_classes)\n",
    "data_YY = to_categorical(data_Y, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape[1:] :  (16, 28)\n",
      "Train on 11 samples, validate on 11 samples\n",
      "Epoch 1/100\n",
      "11/11 [==============================] - 3s 228ms/step - loss: 2.0477 - acc: 0.4830 - val_loss: 6.5402 - val_acc: 0.2614\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.5133 - acc: 0.3239 - val_loss: 2.1634 - val_acc: 0.6364\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.2393 - acc: 0.6023 - val_loss: 2.4032 - val_acc: 0.6080\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1105 - acc: 0.6705 - val_loss: 1.6585 - val_acc: 0.6818\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9260 - acc: 0.6818 - val_loss: 2.4777 - val_acc: 0.2159\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.7956 - acc: 0.6648 - val_loss: 1.3028 - val_acc: 0.8466\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9147 - acc: 0.8295 - val_loss: 2.3166 - val_acc: 0.4943\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5143 - acc: 0.7102 - val_loss: 1.2893 - val_acc: 0.7898\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.4260 - acc: 0.7784 - val_loss: 0.4128 - val_acc: 0.9091\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4313 - acc: 0.9034 - val_loss: 0.3905 - val_acc: 0.9205\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4249 - acc: 0.9205 - val_loss: 0.6728 - val_acc: 0.8750\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.0299 - acc: 0.7557 - val_loss: 0.4987 - val_acc: 0.8409\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4323 - acc: 0.8864 - val_loss: 0.3281 - val_acc: 0.9205\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5335 - acc: 0.8750 - val_loss: 0.3626 - val_acc: 0.9148\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4097 - acc: 0.8977 - val_loss: 0.2943 - val_acc: 0.9148\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.2935 - acc: 0.9205 - val_loss: 0.1969 - val_acc: 0.9602\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.2120 - acc: 0.9545 - val_loss: 0.1777 - val_acc: 0.9602\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0872 - acc: 0.968 - 0s 4ms/step - loss: 0.1956 - acc: 0.9489 - val_loss: 0.1519 - val_acc: 0.9602\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.1754 - acc: 0.9432 - val_loss: 0.1394 - val_acc: 0.9659\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.1876 - acc: 0.9489 - val_loss: 0.3517 - val_acc: 0.9091\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.2667 - acc: 0.9148 - val_loss: 0.4553 - val_acc: 0.9148\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.1995 - acc: 0.9318 - val_loss: 0.3701 - val_acc: 0.9034\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5294 - acc: 0.8523 - val_loss: 0.1551 - val_acc: 0.9602\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.2013 - acc: 0.9432 - val_loss: 0.1234 - val_acc: 0.9545\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.1215 - acc: 0.9602 - val_loss: 0.1055 - val_acc: 0.9716\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.1138 - acc: 0.9602 - val_loss: 0.0968 - val_acc: 0.9716\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.1044 - acc: 0.9659 - val_loss: 0.0870 - val_acc: 0.9659\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.1144 - acc: 0.9489 - val_loss: 0.0771 - val_acc: 0.9716\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0948 - acc: 0.9602 - val_loss: 0.0859 - val_acc: 0.9716\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.1054 - acc: 0.9545 - val_loss: 0.0702 - val_acc: 0.9716\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.0993 - acc: 0.9602 - val_loss: 0.0775 - val_acc: 0.9489\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0824 - acc: 0.9602 - val_loss: 0.1300 - val_acc: 0.9659\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.1126 - acc: 0.9659 - val_loss: 0.1188 - val_acc: 0.9602\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.1054 - acc: 0.9602 - val_loss: 0.1253 - val_acc: 0.9602\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.1799 - acc: 0.9545 - val_loss: 0.1083 - val_acc: 0.9716\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.1163 - acc: 0.9489 - val_loss: 0.0802 - val_acc: 0.9659\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.0953 - acc: 0.9602 - val_loss: 0.0620 - val_acc: 0.9659\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.0913 - acc: 0.9545 - val_loss: 0.0759 - val_acc: 0.9716\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.0948 - acc: 0.9602 - val_loss: 0.0748 - val_acc: 0.9602\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0979 - acc: 0.9545 - val_loss: 0.0613 - val_acc: 0.9716\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0926 - acc: 0.9545 - val_loss: 0.0623 - val_acc: 0.9716\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.0846 - acc: 0.9489 - val_loss: 0.0712 - val_acc: 0.9716\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0959 - acc: 0.9545 - val_loss: 0.0720 - val_acc: 0.9716\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0968 - acc: 0.9545 - val_loss: 0.0572 - val_acc: 0.9716\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0849 - acc: 0.9602 - val_loss: 0.0655 - val_acc: 0.9659\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0916 - acc: 0.9545 - val_loss: 0.0785 - val_acc: 0.9716\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.0918 - acc: 0.9602 - val_loss: 0.0625 - val_acc: 0.9716\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.0861 - acc: 0.9659 - val_loss: 0.0651 - val_acc: 0.9602\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0880 - acc: 0.9489 - val_loss: 0.0679 - val_acc: 0.9716\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.0887 - acc: 0.9602 - val_loss: 0.0588 - val_acc: 0.9716\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.0859 - acc: 0.9545 - val_loss: 0.0565 - val_acc: 0.9716\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0735 - acc: 0.9659 - val_loss: 0.0615 - val_acc: 0.9716\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.0836 - acc: 0.9659 - val_loss: 0.0638 - val_acc: 0.9602\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0790 - acc: 0.9659 - val_loss: 0.0573 - val_acc: 0.9716\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.0884 - acc: 0.9659 - val_loss: 0.0575 - val_acc: 0.9716\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.1926 - acc: 0.9432 - val_loss: 0.2077 - val_acc: 0.9545\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.2074 - acc: 0.9432 - val_loss: 0.1100 - val_acc: 0.9432\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0988 - acc: 0.9545 - val_loss: 0.0853 - val_acc: 0.9659\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.1073 - acc: 0.9602 - val_loss: 0.1213 - val_acc: 0.9489\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.1406 - acc: 0.9318 - val_loss: 0.0847 - val_acc: 0.9659\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0983 - acc: 0.9545 - val_loss: 0.0597 - val_acc: 0.9716\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 5ms/step - loss: 0.0773 - acc: 0.9659 - val_loss: 0.0550 - val_acc: 0.9716\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0735 - acc: 0.9659 - val_loss: 0.0537 - val_acc: 0.9716\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0766 - acc: 0.9545 - val_loss: 0.0622 - val_acc: 0.9716\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0851 - acc: 0.9545 - val_loss: 0.0555 - val_acc: 0.9716\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0688 - acc: 0.9659 - val_loss: 0.0533 - val_acc: 0.9716\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0806 - acc: 0.9602 - val_loss: 0.0676 - val_acc: 0.9716\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0770 - acc: 0.9716 - val_loss: 0.0613 - val_acc: 0.9716\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0838 - acc: 0.9545 - val_loss: 0.0623 - val_acc: 0.9716\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.0782 - acc: 0.9602 - val_loss: 0.0557 - val_acc: 0.9716\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0790 - acc: 0.9602 - val_loss: 0.0554 - val_acc: 0.9716\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0801 - acc: 0.9545 - val_loss: 0.0626 - val_acc: 0.9716\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0818 - acc: 0.9602 - val_loss: 0.0613 - val_acc: 0.9716\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0796 - acc: 0.9659 - val_loss: 0.0547 - val_acc: 0.9716\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.0782 - acc: 0.9602 - val_loss: 0.0585 - val_acc: 0.9602\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0697 - acc: 0.9659 - val_loss: 0.0586 - val_acc: 0.9716\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0953 - acc: 0.9659 - val_loss: 0.0525 - val_acc: 0.9716\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0777 - acc: 0.9602 - val_loss: 0.0794 - val_acc: 0.9659\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.1764 - acc: 0.9545 - val_loss: 0.0657 - val_acc: 0.9716\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.0831 - acc: 0.9659 - val_loss: 0.0644 - val_acc: 0.9716\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0833 - acc: 0.9602 - val_loss: 0.0544 - val_acc: 0.9716\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0749 - acc: 0.9716 - val_loss: 0.0532 - val_acc: 0.9716\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.0779 - acc: 0.9545 - val_loss: 0.0539 - val_acc: 0.9716\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0744 - acc: 0.9545 - val_loss: 0.0567 - val_acc: 0.9716\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.0843 - acc: 0.9602 - val_loss: 0.0514 - val_acc: 0.9716\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0728 - acc: 0.9716 - val_loss: 0.0572 - val_acc: 0.9602\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0852 - acc: 0.9659 - val_loss: 0.0687 - val_acc: 0.9716\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.0758 - acc: 0.9659 - val_loss: 0.0575 - val_acc: 0.9716\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0741 - acc: 0.9659 - val_loss: 0.0683 - val_acc: 0.9716\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0840 - acc: 0.9659 - val_loss: 0.0554 - val_acc: 0.9716\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0768 - acc: 0.9602 - val_loss: 0.0551 - val_acc: 0.9716\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0732 - acc: 0.9659 - val_loss: 0.0588 - val_acc: 0.9716\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0844 - acc: 0.9545 - val_loss: 0.0535 - val_acc: 0.9716\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0756 - acc: 0.9659 - val_loss: 0.0627 - val_acc: 0.9716\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0834 - acc: 0.9602 - val_loss: 0.0600 - val_acc: 0.9716\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0823 - acc: 0.9659 - val_loss: 0.0521 - val_acc: 0.9716\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.0823 - acc: 0.9545 - val_loss: 0.0558 - val_acc: 0.9716\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.0839 - acc: 0.9602 - val_loss: 0.0606 - val_acc: 0.9716\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0773 - acc: 0.9659 - val_loss: 0.1047 - val_acc: 0.9659\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0921 - acc: 0.9545 - val_loss: 0.1409 - val_acc: 0.9432\n",
      "11/11 [==============================] - 0s 314us/step\n",
      "SimpleRNN test loss    :  0.14086852967739105\n",
      "SimpleRNN test accuracy:  0.9431818127632141\n"
     ]
    }
   ],
   "source": [
    "# import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import SimpleRNN\n",
    "from keras import initializers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "hidden_units = 100\n",
    "batch_size = 2\n",
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "x_train = data_XX\n",
    "y_train = data_YY\n",
    "\n",
    "x_test = data_XX\n",
    "y_test = data_YY\n",
    "\n",
    "print(\"x_train.shape[1:] : \", x_train.shape[1:])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(hidden_units,\n",
    "                    return_sequences=True,\n",
    "                    input_shape=x_train.shape[1:]))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "rmsprop = RMSprop(lr=learning_rate)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=rmsprop,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "print('SimpleRNN test loss    : ', scores[0])\n",
    "print('SimpleRNN test accuracy: ', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.23311054e-05 1.42497711e-05 5.24604926e-04 9.90189254e-01\n",
      " 4.08342807e-03 1.23809476e-03 2.31980652e-04 7.36791582e-04\n",
      " 9.43565974e-04 3.02513654e-04 3.37153119e-06 1.98489238e-06\n",
      " 9.60362435e-04 4.73743557e-06 6.57414901e-04 1.75848027e-06\n",
      " 3.08952121e-06 4.70632074e-07 1.58724322e-06 1.23103109e-06\n",
      " 1.06868647e-05 1.01748828e-05 4.48116180e-06 3.09535471e-06\n",
      " 1.45168615e-05 5.21358470e-06 2.72324746e-06 2.62664907e-05]\n",
      "3\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "3\n",
      "PRED:\n",
      "[[ 3  7  5  9  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  7  5  9  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  7  5  9  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [15  2  4 16  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 4 11  8  7  5  4 20 21  0  0  0  0  0  0  0  0]\n",
      " [ 4  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  8  6  4  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  7  6 12  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  8  6  4  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [13 18 13 19  2  5  4 20 21 22 23 10  5  4 20  0]\n",
      " [ 4 25 26  2  5 27  1  0  0  0  0  0  0  0  0  0]]\n",
      "REF :\n",
      "[[ 3  7  2  9  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  7 14  9  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  7  5  9  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [15  2  4 16  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 4 11  8  7  5  4  1  0  0  0  0  0  0  0  0  0]\n",
      " [ 4  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  8  6 12  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  8  6 12  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  8  6  4  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [13 18 13 19  2  5  4 20 21 22 23 10  3 24  1  0]\n",
      " [ 3 25 26  2  5 27  1  0  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "pred_x_test = model.predict(x_test)\n",
    "\n",
    "print(pred_x_test[0][0])\n",
    "print(np.argmax(pred_x_test[0][0]))\n",
    "\n",
    "print(y_test[0][0])\n",
    "print(np.argmax(y_test[0][0]))\n",
    "\n",
    "print(\"PRED:\")\n",
    "print(np.argmax(pred_x_test, axis=-1))\n",
    "\n",
    "print(\"REF :\")\n",
    "print(np.argmax(y_test, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape[1:] :  (16, 28)\n",
      "Train on 11 samples, validate on 11 samples\n",
      "Epoch 1/100\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 2.2859 - acc: 0.5114 - val_loss: 1.2626 - val_acc: 0.6591\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1.3635 - acc: 0.6705 - val_loss: 1.2434 - val_acc: 0.6591\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.2323 - acc: 0.6648 - val_loss: 1.0766 - val_acc: 0.6932\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 1.2095 - acc: 0.7102 - val_loss: 1.0731 - val_acc: 0.7045\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 1.0652 - acc: 0.7102 - val_loss: 1.2449 - val_acc: 0.6932\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.2222 - acc: 0.7102 - val_loss: 1.0874 - val_acc: 0.8239\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.9755 - acc: 0.8011 - val_loss: 0.8631 - val_acc: 0.8239\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.8555 - acc: 0.8125 - val_loss: 0.7686 - val_acc: 0.8239\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.7893 - acc: 0.8295 - val_loss: 0.7657 - val_acc: 0.8125\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.6908 - acc: 0.8580 - val_loss: 0.6491 - val_acc: 0.8580\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.6836 - acc: 0.8409 - val_loss: 0.6722 - val_acc: 0.8523\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.6751 - acc: 0.8523 - val_loss: 0.5346 - val_acc: 0.8750\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.5740 - acc: 0.8580 - val_loss: 0.4743 - val_acc: 0.8750\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.4778 - acc: 0.8636 - val_loss: 0.3969 - val_acc: 0.9034\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.4027 - acc: 0.8920 - val_loss: 0.3467 - val_acc: 0.9148\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.3549 - acc: 0.9261 - val_loss: 0.3696 - val_acc: 0.8864\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3976 - acc: 0.8750 - val_loss: 0.3564 - val_acc: 0.9034\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.3678 - acc: 0.9091 - val_loss: 0.3458 - val_acc: 0.9148\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.3246 - acc: 0.9091 - val_loss: 0.2825 - val_acc: 0.9205\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.3099 - acc: 0.9261 - val_loss: 0.4808 - val_acc: 0.8636\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.5027 - acc: 0.8636 - val_loss: 0.4005 - val_acc: 0.8750\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.4043 - acc: 0.8693 - val_loss: 0.1942 - val_acc: 0.9545\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.2055 - acc: 0.9432 - val_loss: 0.1640 - val_acc: 0.9602\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1767 - acc: 0.9602 - val_loss: 0.1474 - val_acc: 0.9602\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1636 - acc: 0.9545 - val_loss: 0.1720 - val_acc: 0.9375\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1826 - acc: 0.9375 - val_loss: 0.2593 - val_acc: 0.9148\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.2781 - acc: 0.8977 - val_loss: 0.1712 - val_acc: 0.9375\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1769 - acc: 0.9375 - val_loss: 0.1309 - val_acc: 0.9659\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1530 - acc: 0.9545 - val_loss: 0.1161 - val_acc: 0.9716\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1277 - acc: 0.9716 - val_loss: 0.1061 - val_acc: 0.9716\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1211 - acc: 0.9602 - val_loss: 0.1010 - val_acc: 0.9716\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1123 - acc: 0.9659 - val_loss: 0.0899 - val_acc: 0.9602\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0982 - acc: 0.9659 - val_loss: 0.1177 - val_acc: 0.9545\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1261 - acc: 0.9489 - val_loss: 0.1418 - val_acc: 0.9375\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1539 - acc: 0.9375 - val_loss: 0.1062 - val_acc: 0.9602\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1244 - acc: 0.9602 - val_loss: 0.0865 - val_acc: 0.9659\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0977 - acc: 0.9659 - val_loss: 0.0657 - val_acc: 0.9716\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0844 - acc: 0.9602 - val_loss: 0.0615 - val_acc: 0.9716\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0794 - acc: 0.9659 - val_loss: 0.0587 - val_acc: 0.9716\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0820 - acc: 0.9602 - val_loss: 0.0568 - val_acc: 0.9716\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0792 - acc: 0.9659 - val_loss: 0.0645 - val_acc: 0.9716\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0757 - acc: 0.9659 - val_loss: 0.0579 - val_acc: 0.9716\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0788 - acc: 0.9659 - val_loss: 0.0548 - val_acc: 0.9716\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0723 - acc: 0.9659 - val_loss: 0.0536 - val_acc: 0.9716\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0707 - acc: 0.9659 - val_loss: 0.0537 - val_acc: 0.9716\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0625 - acc: 0.9659 - val_loss: 0.0526 - val_acc: 0.9716\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0731 - acc: 0.9659 - val_loss: 0.0614 - val_acc: 0.9602\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0694 - acc: 0.9602 - val_loss: 0.0548 - val_acc: 0.9716\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0658 - acc: 0.9716 - val_loss: 0.0499 - val_acc: 0.9716\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0857 - acc: 0.9602 - val_loss: 0.0515 - val_acc: 0.9716\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0680 - acc: 0.9659 - val_loss: 0.0558 - val_acc: 0.9602\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0686 - acc: 0.9545 - val_loss: 0.0512 - val_acc: 0.9716\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0664 - acc: 0.9659 - val_loss: 0.0527 - val_acc: 0.9716\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0680 - acc: 0.9602 - val_loss: 0.0509 - val_acc: 0.9716\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.0662 - acc: 0.9659 - val_loss: 0.0495 - val_acc: 0.9716\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0643 - acc: 0.9659 - val_loss: 0.0518 - val_acc: 0.9716\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0631 - acc: 0.9716 - val_loss: 0.0497 - val_acc: 0.9716\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0633 - acc: 0.9602 - val_loss: 0.0492 - val_acc: 0.9716\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0651 - acc: 0.9659 - val_loss: 0.0519 - val_acc: 0.9716\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0622 - acc: 0.9659 - val_loss: 0.0495 - val_acc: 0.9716\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0646 - acc: 0.9602 - val_loss: 0.0498 - val_acc: 0.9716\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 13ms/step - loss: 0.0639 - acc: 0.9545 - val_loss: 0.0524 - val_acc: 0.9716\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0659 - acc: 0.9602 - val_loss: 0.0526 - val_acc: 0.9716\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.0646 - acc: 0.9659 - val_loss: 0.0500 - val_acc: 0.9716\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0629 - acc: 0.9716 - val_loss: 0.0501 - val_acc: 0.9716\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0641 - acc: 0.9545 - val_loss: 0.0496 - val_acc: 0.9716\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0613 - acc: 0.9602 - val_loss: 0.0489 - val_acc: 0.9716\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0609 - acc: 0.9716 - val_loss: 0.0484 - val_acc: 0.9716\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0613 - acc: 0.9602 - val_loss: 0.0488 - val_acc: 0.9716\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0601 - acc: 0.9659 - val_loss: 0.0491 - val_acc: 0.9716\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0609 - acc: 0.9716 - val_loss: 0.0486 - val_acc: 0.9716\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0604 - acc: 0.9659 - val_loss: 0.0486 - val_acc: 0.9716\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0587 - acc: 0.9602 - val_loss: 0.0494 - val_acc: 0.9716\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0597 - acc: 0.9602 - val_loss: 0.0499 - val_acc: 0.9716\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0597 - acc: 0.9659 - val_loss: 0.0493 - val_acc: 0.9716\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0597 - acc: 0.9659 - val_loss: 0.0495 - val_acc: 0.9716\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0603 - acc: 0.9659 - val_loss: 0.0496 - val_acc: 0.9716\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0645 - acc: 0.9659 - val_loss: 0.0481 - val_acc: 0.9716\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0593 - acc: 0.9602 - val_loss: 0.0485 - val_acc: 0.9716\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0580 - acc: 0.9716 - val_loss: 0.0479 - val_acc: 0.9716\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0595 - acc: 0.9716 - val_loss: 0.0486 - val_acc: 0.9716\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0588 - acc: 0.9716 - val_loss: 0.0485 - val_acc: 0.9716\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0599 - acc: 0.9659 - val_loss: 0.0489 - val_acc: 0.9716\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0595 - acc: 0.9602 - val_loss: 0.0488 - val_acc: 0.9716\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0586 - acc: 0.9602 - val_loss: 0.0493 - val_acc: 0.9716\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0591 - acc: 0.9659 - val_loss: 0.0481 - val_acc: 0.9716\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0582 - acc: 0.9602 - val_loss: 0.0484 - val_acc: 0.9716\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0577 - acc: 0.9659 - val_loss: 0.0478 - val_acc: 0.9716\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0580 - acc: 0.9659 - val_loss: 0.0483 - val_acc: 0.9716\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0584 - acc: 0.9659 - val_loss: 0.0479 - val_acc: 0.9716\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0586 - acc: 0.9602 - val_loss: 0.0482 - val_acc: 0.9716\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0580 - acc: 0.9716 - val_loss: 0.0484 - val_acc: 0.9716\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0573 - acc: 0.9545 - val_loss: 0.0487 - val_acc: 0.9716\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0577 - acc: 0.9602 - val_loss: 0.0491 - val_acc: 0.9716\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0559 - acc: 0.9659 - val_loss: 0.0498 - val_acc: 0.9716\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0589 - acc: 0.9659 - val_loss: 0.0491 - val_acc: 0.9716\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0551 - acc: 0.9602 - val_loss: 0.0485 - val_acc: 0.9716\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.0583 - acc: 0.9659 - val_loss: 0.0479 - val_acc: 0.9716\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0578 - acc: 0.9659 - val_loss: 0.0477 - val_acc: 0.9716\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0590 - acc: 0.9659 - val_loss: 0.0479 - val_acc: 0.9716\n",
      "11/11 [==============================] - 0s 388us/step\n",
      "LSTM test loss    :  0.04791221395134926\n",
      "LSTM test accuracy:  0.9715909361839294\n"
     ]
    }
   ],
   "source": [
    "# import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras import initializers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "hidden_units = 100\n",
    "batch_size = 2\n",
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "x_train = data_XX\n",
    "y_train = data_YY\n",
    "\n",
    "x_test = data_XX\n",
    "y_test = data_YY\n",
    "\n",
    "print(\"x_train.shape[1:] : \", x_train.shape[1:])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(hidden_units,\n",
    "               return_sequences=True,\n",
    "               input_shape=x_train.shape[1:]))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "rmsprop = RMSprop(lr=learning_rate)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=rmsprop,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "print('LSTM test loss    : ', scores[0])\n",
    "print('LSTM test accuracy: ', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.95732542e-07 4.05812841e-08 1.12221032e-05 9.98842597e-01\n",
      " 7.14005379e-04 1.11933245e-04 4.51965292e-07 1.83859142e-04\n",
      " 9.79927645e-05 2.84290923e-06 2.18841876e-07 4.38682292e-07\n",
      " 1.63867953e-05 1.09819290e-07 1.45747044e-05 3.65063414e-07\n",
      " 2.70845391e-07 1.99678965e-07 2.10844490e-07 3.10007636e-07\n",
      " 1.63577525e-07 2.09492583e-07 3.45573113e-07 3.83136694e-07\n",
      " 2.30446020e-07 3.04655279e-09 3.06173504e-08 8.73914061e-08]\n",
      "3\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "3\n",
      "PRED:\n",
      "[[ 3  7  5  9  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  7  5  9  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  7  5  9  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [15  2  4 16  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3 11  8  7  5  4  1  0  0  0  0  0  0  0  0  0]\n",
      " [ 4  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  8  6  4  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  7  6 12  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  8  6  4  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [13 18 13 19  2  5  4 20 21 22 23 10  3 24  1  0]\n",
      " [ 3 25 26  2  5 27  1  0  0  0  0  0  0  0  0  0]]\n",
      "REF :\n",
      "[[ 3  7  2  9  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  7 14  9  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  7  5  9  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [15  2  4 16  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 4 11  8  7  5  4  1  0  0  0  0  0  0  0  0  0]\n",
      " [ 4  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  8  6 12  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  8  6 12  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  8  6  4  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [13 18 13 19  2  5  4 20 21 22 23 10  3 24  1  0]\n",
      " [ 3 25 26  2  5 27  1  0  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "pred_x_test = model.predict(x_test)\n",
    "\n",
    "print(pred_x_test[0][0])\n",
    "print(np.argmax(pred_x_test[0][0]))\n",
    "\n",
    "print(y_test[0][0])\n",
    "print(np.argmax(y_test[0][0]))\n",
    "\n",
    "print(\"PRED:\")\n",
    "print(np.argmax(pred_x_test, axis=-1))\n",
    "\n",
    "print(\"REF :\")\n",
    "print(np.argmax(y_test, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape[1:] :  (16,)\n",
      "Train on 11 samples, validate on 11 samples\n",
      "Epoch 1/100\n",
      "11/11 [==============================] - 3s 306ms/step - loss: 2.1292 - acc: 0.5114 - val_loss: 1.5838 - val_acc: 0.6307\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 1.8752 - acc: 0.5170 - val_loss: 1.2844 - val_acc: 0.6307\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.3394 - acc: 0.6307 - val_loss: 1.5068 - val_acc: 0.6534\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.3665 - acc: 0.6705 - val_loss: 1.1752 - val_acc: 0.6364\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.1154 - acc: 0.6761 - val_loss: 1.0076 - val_acc: 0.6875\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 1.0461 - acc: 0.7045 - val_loss: 0.9564 - val_acc: 0.7500\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.9501 - acc: 0.7500 - val_loss: 0.8839 - val_acc: 0.7784\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.8962 - acc: 0.7500 - val_loss: 0.8408 - val_acc: 0.7557\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.8875 - acc: 0.7557 - val_loss: 1.0486 - val_acc: 0.6932\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 1.0857 - acc: 0.6591 - val_loss: 0.7850 - val_acc: 0.8125\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.7963 - acc: 0.7784 - val_loss: 0.6595 - val_acc: 0.8011\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.6735 - acc: 0.7898 - val_loss: 0.5847 - val_acc: 0.8068\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.6349 - acc: 0.8011 - val_loss: 0.7243 - val_acc: 0.7841\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.6596 - acc: 0.7898 - val_loss: 0.5238 - val_acc: 0.8182\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.5436 - acc: 0.8125 - val_loss: 0.4768 - val_acc: 0.8523\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.5036 - acc: 0.8295 - val_loss: 0.6113 - val_acc: 0.8295\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 0.6270 - acc: 0.8295 - val_loss: 0.4553 - val_acc: 0.8409\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.4652 - acc: 0.8182 - val_loss: 0.4003 - val_acc: 0.8750\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.4280 - acc: 0.8750 - val_loss: 0.3744 - val_acc: 0.8750\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.3828 - acc: 0.8636 - val_loss: 0.2913 - val_acc: 0.8864\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.3225 - acc: 0.8807 - val_loss: 0.2592 - val_acc: 0.9148\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.2809 - acc: 0.8920 - val_loss: 0.2587 - val_acc: 0.9148\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.2665 - acc: 0.9091 - val_loss: 0.2594 - val_acc: 0.9034\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 0.2776 - acc: 0.8920 - val_loss: 0.2620 - val_acc: 0.8977\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.2568 - acc: 0.8977 - val_loss: 0.1719 - val_acc: 0.9489\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.2032 - acc: 0.9375 - val_loss: 0.1542 - val_acc: 0.9545\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.1695 - acc: 0.9375 - val_loss: 0.1130 - val_acc: 0.9659\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.1341 - acc: 0.9489 - val_loss: 0.1127 - val_acc: 0.9545\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.1253 - acc: 0.9489 - val_loss: 0.1309 - val_acc: 0.9545\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.1415 - acc: 0.9602 - val_loss: 0.1508 - val_acc: 0.9489\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.1598 - acc: 0.9432 - val_loss: 0.0875 - val_acc: 0.9659\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.1050 - acc: 0.9489 - val_loss: 0.0787 - val_acc: 0.9659\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.0941 - acc: 0.9545 - val_loss: 0.0714 - val_acc: 0.9659\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.0883 - acc: 0.9545 - val_loss: 0.0676 - val_acc: 0.9716\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.0812 - acc: 0.9602 - val_loss: 0.0695 - val_acc: 0.9659\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.0889 - acc: 0.9545 - val_loss: 0.0644 - val_acc: 0.9716\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.0795 - acc: 0.9659 - val_loss: 0.0603 - val_acc: 0.9716\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.0727 - acc: 0.9659 - val_loss: 0.0549 - val_acc: 0.9716\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.0757 - acc: 0.9602 - val_loss: 0.0581 - val_acc: 0.9716\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.0754 - acc: 0.9602 - val_loss: 0.0550 - val_acc: 0.9716\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 0.0770 - acc: 0.9489 - val_loss: 0.0540 - val_acc: 0.9716\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.0708 - acc: 0.9659 - val_loss: 0.0582 - val_acc: 0.9602\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.0740 - acc: 0.9545 - val_loss: 0.0556 - val_acc: 0.9716\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.0652 - acc: 0.9659 - val_loss: 0.0539 - val_acc: 0.9716\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.0685 - acc: 0.9602 - val_loss: 0.0526 - val_acc: 0.9716\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.0651 - acc: 0.9545 - val_loss: 0.0506 - val_acc: 0.9716\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.0685 - acc: 0.9602 - val_loss: 0.0545 - val_acc: 0.9716\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 0.0778 - acc: 0.9716 - val_loss: 0.0509 - val_acc: 0.9716\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 0.1051 - acc: 0.9602 - val_loss: 0.0573 - val_acc: 0.9716\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.0724 - acc: 0.9716 - val_loss: 0.0508 - val_acc: 0.9716\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.0667 - acc: 0.9659 - val_loss: 0.0497 - val_acc: 0.9716\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.0679 - acc: 0.9602 - val_loss: 0.0526 - val_acc: 0.9716\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.0605 - acc: 0.9659 - val_loss: 0.0505 - val_acc: 0.9716\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.0665 - acc: 0.9659 - val_loss: 0.0490 - val_acc: 0.9716\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.0612 - acc: 0.9659 - val_loss: 0.0495 - val_acc: 0.9716\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.0601 - acc: 0.9716 - val_loss: 0.0503 - val_acc: 0.9716\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.0643 - acc: 0.9602 - val_loss: 0.0489 - val_acc: 0.9716\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.0604 - acc: 0.9659 - val_loss: 0.0486 - val_acc: 0.9716\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.0639 - acc: 0.9659 - val_loss: 0.0531 - val_acc: 0.9716\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.0930 - acc: 0.9659 - val_loss: 0.0677 - val_acc: 0.9659\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.0835 - acc: 0.9545 - val_loss: 0.0512 - val_acc: 0.9716\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 20ms/step - loss: 0.0638 - acc: 0.9659 - val_loss: 0.0484 - val_acc: 0.9716\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 0s 26ms/step - loss: 0.0611 - acc: 0.9602 - val_loss: 0.0497 - val_acc: 0.9716\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.0601 - acc: 0.9602 - val_loss: 0.0488 - val_acc: 0.9716\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.0565 - acc: 0.9716 - val_loss: 0.0480 - val_acc: 0.9716\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.0608 - acc: 0.9602 - val_loss: 0.0496 - val_acc: 0.9716\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.0605 - acc: 0.9659 - val_loss: 0.0485 - val_acc: 0.9716\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.0598 - acc: 0.9659 - val_loss: 0.0485 - val_acc: 0.9716\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.0599 - acc: 0.9602 - val_loss: 0.0495 - val_acc: 0.9716\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.0587 - acc: 0.9602 - val_loss: 0.0494 - val_acc: 0.9716\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.0854 - acc: 0.9716 - val_loss: 0.0525 - val_acc: 0.9716\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.0764 - acc: 0.9602 - val_loss: 0.0509 - val_acc: 0.9716\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.0631 - acc: 0.9716 - val_loss: 0.0483 - val_acc: 0.9716\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0597 - acc: 0.9659 - val_loss: 0.0491 - val_acc: 0.9716\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 0.0598 - acc: 0.9659 - val_loss: 0.0486 - val_acc: 0.9716\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.0590 - acc: 0.9602 - val_loss: 0.0486 - val_acc: 0.9716\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.0584 - acc: 0.9716 - val_loss: 0.0489 - val_acc: 0.9716\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.0595 - acc: 0.9659 - val_loss: 0.0486 - val_acc: 0.9716\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 0.0585 - acc: 0.9659 - val_loss: 0.0483 - val_acc: 0.9716\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.0605 - acc: 0.9602 - val_loss: 0.0491 - val_acc: 0.9716\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.0579 - acc: 0.9659 - val_loss: 0.0483 - val_acc: 0.9716\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.0647 - acc: 0.9602 - val_loss: 0.0493 - val_acc: 0.9716\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.0588 - acc: 0.9659 - val_loss: 0.0532 - val_acc: 0.9716\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.0627 - acc: 0.9659 - val_loss: 0.0506 - val_acc: 0.9716\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.0575 - acc: 0.9659 - val_loss: 0.0506 - val_acc: 0.9716\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.0590 - acc: 0.9716 - val_loss: 0.0492 - val_acc: 0.9716\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.0572 - acc: 0.9602 - val_loss: 0.0483 - val_acc: 0.9716\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.0570 - acc: 0.9659 - val_loss: 0.0488 - val_acc: 0.9716\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.0523 - acc: 0.9659 - val_loss: 0.0483 - val_acc: 0.9716\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.0573 - acc: 0.9602 - val_loss: 0.0478 - val_acc: 0.9716\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 0.0564 - acc: 0.9659 - val_loss: 0.0485 - val_acc: 0.9716\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.0547 - acc: 0.9716 - val_loss: 0.0479 - val_acc: 0.9716\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.0558 - acc: 0.9659 - val_loss: 0.0486 - val_acc: 0.9716\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 0.0569 - acc: 0.9545 - val_loss: 0.0483 - val_acc: 0.9716\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.0608 - acc: 0.9602 - val_loss: 0.0481 - val_acc: 0.9716\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.0532 - acc: 0.9716 - val_loss: 0.0484 - val_acc: 0.9716\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.0561 - acc: 0.9659 - val_loss: 0.0482 - val_acc: 0.9716\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.0549 - acc: 0.9659 - val_loss: 0.0485 - val_acc: 0.9716\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.0553 - acc: 0.9659 - val_loss: 0.0484 - val_acc: 0.9716\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.0551 - acc: 0.9602 - val_loss: 0.0481 - val_acc: 0.9716\n",
      "11/11 [==============================] - 0s 593us/step\n",
      "LSTM test loss    :  0.04814881086349487\n",
      "LSTM test accuracy:  0.9715909361839294\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 16, 100)           2800      \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 16, 100)           80400     \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, 16, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 16, 28)            2828      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 16, 28)            0         \n",
      "=================================================================\n",
      "Total params: 166,428\n",
      "Trainable params: 166,428\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM, Embedding\n",
    "from keras import initializers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "word_embedding_size = 100\n",
    "vocab_size = len(vocab_dict) + 1\n",
    "\n",
    "hidden_units = 100\n",
    "batch_size = 2\n",
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "x_train = data_X\n",
    "y_train = data_YY\n",
    "\n",
    "x_test = data_X\n",
    "y_test = data_YY\n",
    "\n",
    "print(\"x_train.shape[1:] : \", x_train.shape[1:])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, word_embedding_size, input_length=x_train.shape[1]))\n",
    "# the model will take as input an integer matrix of size (batch, input_length).\n",
    "# the largest integer (i.e. word index) in the input should be\n",
    "# no larger than 999 (vocabulary size).\n",
    "# now model.output_shape == (None, 10, 64), where None is the batch dimension.\n",
    "\n",
    "model.add(LSTM(hidden_units,\n",
    "               return_sequences=True,\n",
    "               input_shape=x_train.shape[1:]))\n",
    "model.add(LSTM(hidden_units,\n",
    "               return_sequences=True))\n",
    "               # input_shape=x_train.shape[1:]))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "rmsprop = RMSprop(lr=learning_rate)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=rmsprop,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "print('LSTM test loss    : ', scores[0])\n",
    "print('LSTM test accuracy: ', scores[1])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.4989556e-06 4.3600053e-08 2.3447221e-05 9.9939024e-01 5.2770972e-04\n",
      " 1.9750378e-05 7.6117551e-08 3.6144820e-06 5.6358299e-06 4.1778785e-07\n",
      " 1.3648093e-07 2.0916534e-07 9.1166921e-06 2.5333975e-09 1.5537978e-05\n",
      " 2.4682583e-07 2.1015076e-08 1.6741654e-07 1.5881608e-07 2.3291724e-07\n",
      " 9.8057846e-08 1.1981598e-07 5.5200534e-08 2.8253012e-07 5.2715837e-08\n",
      " 1.1915068e-08 2.6876538e-08 3.3931027e-08]\n",
      "3\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "3\n",
      "PRED:\n",
      "[[ 3  7  2  9  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  7  2  9  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  7  2  9  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [15  2  4 16  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3 11  8  7  5  4  1  0  0  0  0  0  0  0  0  0]\n",
      " [ 4  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  8  6 12  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  7  6 12  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  8  6 12  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [13 18 13 19  2  5  4 20 21 22 23 10  3 24  1  0]\n",
      " [ 3 25 26  2  5 27  1  0  0  0  0  0  0  0  0  0]]\n",
      "REF :\n",
      "[[ 3  7  2  9  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  7 14  9  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  7  5  9  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [15  2  4 16  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 4 11  8  7  5  4  1  0  0  0  0  0  0  0  0  0]\n",
      " [ 4  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  8  6 12  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  8  6 12  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  8  6  4  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [13 18 13 19  2  5  4 20 21 22 23 10  3 24  1  0]\n",
      " [ 3 25 26  2  5 27  1  0  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "pred_x_test = model.predict(x_test)\n",
    "\n",
    "print(pred_x_test[0][0])\n",
    "print(np.argmax(pred_x_test[0][0]))\n",
    "\n",
    "print(y_test[0][0])\n",
    "print(np.argmax(y_test[0][0]))\n",
    "\n",
    "print(\"PRED:\")\n",
    "print(np.argmax(pred_x_test, axis=-1))\n",
    "\n",
    "print(\"REF :\")\n",
    "print(np.argmax(y_test, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.embeddings.Embedding object at 0x7f802528d780>\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0])\n",
    "emebdding_layer = model.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': array([-0.13778307,  0.84110916,  0.66433126,  0.11956102,  0.32808262,\n",
      "       -0.38334334, -0.28741267, -0.4025532 ,  0.7828784 ,  0.05417778,\n",
      "        0.5828485 , -0.4213726 , -0.40576053, -0.04384682,  0.19760506,\n",
      "       -0.3636471 , -0.25753638,  0.35240605, -0.31055078,  0.13211852,\n",
      "        0.5220053 ,  0.31980762, -0.33987084,  0.31706208,  0.1601418 ,\n",
      "       -0.73442745, -0.24030505, -0.47462854,  0.29781175,  0.07220611,\n",
      "       -0.01764157,  0.34051502, -0.02193841, -0.19588548, -0.25769347,\n",
      "        0.55673015,  0.27762407,  0.51636773, -0.46597013, -0.40896323,\n",
      "       -0.04641417,  0.30810094,  0.2532996 , -0.19441861, -0.26800898,\n",
      "       -0.5372454 ,  0.09823105,  0.5135554 , -0.24092518,  0.20084311,\n",
      "       -0.5397324 ,  0.07274378,  0.33876082, -0.3986542 ,  0.7805507 ,\n",
      "        0.55476636, -0.83242846,  0.40878826, -0.05832305,  0.33182076,\n",
      "        0.40938106, -0.43098304,  0.7138856 ,  0.29883102, -0.3523413 ,\n",
      "        0.20672964, -0.16559055, -0.6675453 , -0.50566524,  0.27678025,\n",
      "       -0.46045086, -0.04207763, -0.28237662,  0.5687304 , -0.42792326,\n",
      "       -0.01101308, -0.38895994,  0.33273515, -0.6487469 ,  0.31359667,\n",
      "       -0.4153592 , -0.47135082,  0.7693768 , -0.39754313, -0.5473516 ,\n",
      "       -0.5828261 ,  0.67326164, -0.14035329,  0.38695583,  0.20802349,\n",
      "        0.47069862, -0.23936437, -0.532318  ,  0.02747663, -0.61082745,\n",
      "        0.43396595,  0.29379752,  0.17505093, -0.65618294, -0.4555755 ],\n",
      "      dtype=float32), 'a': array([-0.06875994, -0.20833431,  0.04123373,  0.12888315, -0.03128641,\n",
      "       -0.25832862,  0.2156725 ,  0.32424814, -0.399515  ,  0.19794017,\n",
      "       -0.16954531,  0.45997262,  0.36665908,  0.3683005 , -0.01506904,\n",
      "       -0.04653837,  0.14776587, -0.00315037,  0.1604945 ,  0.06328477,\n",
      "       -0.40676546, -0.38785556, -0.13179006,  0.08967017,  0.05352709,\n",
      "        0.1166191 , -0.122819  ,  0.54076433, -0.23366491, -0.10063681,\n",
      "       -0.13875872, -0.20387217,  0.5015072 ,  0.06559767, -0.01833885,\n",
      "       -0.17275022, -0.38747045, -0.44066006,  0.0719465 , -0.15870135,\n",
      "       -0.02581861, -0.02386496,  0.07602523, -0.10299641,  0.25695142,\n",
      "        0.04242549, -0.05370153, -0.45561346, -0.04554815, -0.41178906,\n",
      "        0.10241396, -0.2519498 , -0.1270956 ,  0.20824441, -0.3524509 ,\n",
      "       -0.24101143,  0.07780015, -0.3577633 , -0.07624809,  0.02442324,\n",
      "       -0.36163267,  0.34982044, -0.15202758, -0.23856293,  0.26145875,\n",
      "        0.08328701,  0.207449  ,  0.31465968,  0.46622244, -0.24014495,\n",
      "        0.18200724,  0.1513361 ,  0.09931161, -0.31541044,  0.27760515,\n",
      "       -0.25348526,  0.32119122,  0.11459783,  0.16312091,  0.00771792,\n",
      "        0.30842125,  0.00653207, -0.22903514,  0.10914662,  0.14171885,\n",
      "        0.22392556, -0.29660258,  0.29875758, -0.14159219,  0.03609094,\n",
      "       -0.38753065, -0.05664884,  0.12226041, -0.21896987, -0.1243434 ,\n",
      "       -0.20572752,  0.03564474, -0.15318404,  0.20203565,  0.09440829],\n",
      "      dtype=float32), 'barber': array([-0.08521013,  0.04183844, -0.10600397,  0.2816236 , -0.08247718,\n",
      "       -0.12669058,  0.072425  , -0.08257344, -0.01619127,  0.09917671,\n",
      "       -0.03506725, -0.13299862, -0.01912012, -0.12782875, -0.05363728,\n",
      "        0.03723474, -0.08110777, -0.00920613,  0.06212485,  0.11659178,\n",
      "        0.21787561, -0.195717  , -0.18953079, -0.02364411,  0.33918756,\n",
      "        0.12524231,  0.04668527,  0.05752857, -0.07381717, -0.09909495,\n",
      "       -0.11367676, -0.00623834, -0.09746572, -0.27338842, -0.14604788,\n",
      "       -0.0295278 ,  0.04513305,  0.11273925,  0.04954884, -0.0967645 ,\n",
      "       -0.2960083 ,  0.0342954 ,  0.02681018, -0.20370786, -0.01151046,\n",
      "        0.04244458,  0.16725494,  0.0385061 ,  0.00664611, -0.27164692,\n",
      "        0.0967521 , -0.1557534 , -0.08011765,  0.0273912 ,  0.07536416,\n",
      "       -0.04079482, -0.00194262,  0.16298161, -0.05316889,  0.03259494,\n",
      "        0.05646679, -0.06862847, -0.10371475,  0.26638785, -0.04162062,\n",
      "        0.25412524, -0.1480218 , -0.046637  , -0.00548266,  0.01670582,\n",
      "       -0.10419339, -0.09639233,  0.09584246,  0.04768578, -0.00742401,\n",
      "       -0.00079265, -0.08439282,  0.09644403,  0.16669592, -0.20335813,\n",
      "       -0.21905722, -0.09512816, -0.02960181, -0.02814782,  0.04662682,\n",
      "       -0.06571402, -0.01087444, -0.10929782,  0.10362708,  0.18473747,\n",
      "       -0.0638132 , -0.07492815, -0.04549425,  0.1719871 ,  0.01720976,\n",
      "        0.01202237,  0.02022345,  0.10367794,  0.13947898, -0.1181339 ],\n",
      "      dtype=float32), 'secret': array([-0.04966722,  0.10510977,  0.16171718,  0.0291101 ,  0.19315793,\n",
      "        0.33420506, -0.13253325, -0.10805335,  0.0106641 ,  0.3916203 ,\n",
      "        0.08516974, -0.17350796, -0.04249657, -0.01977581,  0.21889795,\n",
      "       -0.11917561,  0.00751068,  0.11114524, -0.16898122,  0.1272912 ,\n",
      "       -0.03510062,  0.14167011,  0.06023655,  0.23635128,  0.064969  ,\n",
      "       -0.16385506, -0.19170454, -0.10148809,  0.19036962, -0.20583205,\n",
      "       -0.18055494,  0.20043612, -0.11638936, -0.32128826, -0.10331378,\n",
      "        0.05956586,  0.24459805,  0.11076272, -0.20238017, -0.28612974,\n",
      "       -0.14599091,  0.37336925,  0.29779857, -0.05925386, -0.17176126,\n",
      "       -0.22468328, -0.02690824,  0.00182446, -0.06880505,  0.03415816,\n",
      "       -0.17526719, -0.13083218,  0.23352955, -0.04592791,  0.09266422,\n",
      "        0.15174504, -0.06646712,  0.14207286,  0.16702485,  0.16554895,\n",
      "        0.09939471, -0.18903099,  0.09623059,  0.21521904, -0.29122368,\n",
      "        0.1683004 , -0.1853381 , -0.02115178, -0.11696516,  0.23411618,\n",
      "       -0.29897684, -0.17363025, -0.02682149,  0.21558054, -0.09997404,\n",
      "        0.23187219, -0.12972653,  0.02114506, -0.06790195,  0.03494408,\n",
      "       -0.07439063, -0.36520392, -0.07848428, -0.28668553, -0.1405517 ,\n",
      "       -0.18248121,  0.12842378, -0.32330757, -0.0276359 ,  0.13523348,\n",
      "        0.1336271 , -0.0770408 , -0.16458255,  0.15490782,  0.10784604,\n",
      "        0.14755283,  0.23385404,  0.32489043,  0.03273031, -0.30383703],\n",
      "      dtype=float32), 'huge': array([-0.53875935, -0.18313378,  0.39041597,  0.15751798,  0.52274346,\n",
      "       -0.00946545, -0.25000125,  1.1156645 ,  0.15979764, -0.43711466,\n",
      "       -0.0612926 ,  0.144647  ,  0.18459326,  0.5801836 ,  0.5084952 ,\n",
      "       -0.69673693, -0.2931678 ,  0.48814586, -0.12823036,  0.30533746,\n",
      "       -0.4416845 ,  0.00623675, -0.08273195,  0.5389494 , -0.20091079,\n",
      "        0.25197053, -0.12623106, -0.03070527,  0.24897131,  0.03076914,\n",
      "       -0.08991865, -0.03916629,  0.31266317,  0.56788564, -0.4182643 ,\n",
      "        0.49636403, -0.0556758 ,  0.16543132, -0.1283173 ,  0.20775892,\n",
      "        0.12251213, -0.46660024,  0.23038486,  0.8270511 ,  0.24183968,\n",
      "       -0.5231109 ,  0.7704493 ,  0.11242283,  0.7172576 ,  0.3030224 ,\n",
      "       -0.38529202, -0.30229235,  0.14467818,  0.09758682, -0.01337075,\n",
      "        0.19849727, -0.00922451, -0.26433608,  0.09419297,  0.56864405,\n",
      "        0.14604896, -0.07782295,  0.45340616, -0.11149226,  0.521637  ,\n",
      "       -0.506787  ,  0.06299984, -0.1359104 ,  0.19605231,  0.17152235,\n",
      "       -0.38340232,  0.6770565 , -0.67818356, -0.5107321 , -0.23906556,\n",
      "        0.07243486, -0.14982016,  0.09240488,  0.9072255 , -0.594836  ,\n",
      "        0.19988114, -0.05751052,  0.35179004,  0.04074552, -0.33996654,\n",
      "        0.0199397 , -0.12805979, -0.30189815, -0.08307129,  0.12278646,\n",
      "        0.145838  , -0.42125693,  0.18560898, -0.4277378 ,  0.0406477 ,\n",
      "       -0.19235206,  0.468425  , -0.33201802,  0.04366932, -0.12769629],\n",
      "      dtype=float32), 'his': array([-7.52851665e-02, -1.51494429e-01, -2.58568466e-01,  1.07685216e-01,\n",
      "       -6.00696802e-02, -1.70312732e-01, -7.02522397e-02, -3.20588887e-01,\n",
      "       -1.75065160e-01,  3.49033922e-02,  3.13255847e-01,  1.86402887e-01,\n",
      "        1.53825104e-01, -5.84068857e-02, -5.39593279e-01,  6.16100095e-02,\n",
      "       -4.15170193e-02,  1.52706176e-01, -3.88158076e-02,  3.68858352e-02,\n",
      "       -9.19534042e-02,  3.70385982e-02, -1.66875109e-01, -1.08644061e-01,\n",
      "        1.60699978e-01,  6.97268963e-01, -3.45886238e-02,  1.35224670e-01,\n",
      "       -5.84594645e-02, -1.25608414e-01, -1.64139986e-01,  2.46539302e-02,\n",
      "       -2.69455835e-02,  4.93683845e-01, -1.64804012e-02, -3.23791429e-02,\n",
      "       -2.20475540e-01, -7.14113861e-02,  7.91306421e-02, -1.15826264e-01,\n",
      "       -1.26697812e-02,  1.87270176e-02,  3.59017774e-02, -2.18663603e-01,\n",
      "        1.47888273e-01, -3.56892571e-02, -1.82009246e-02, -1.11432463e-01,\n",
      "        5.79600215e-01, -1.44473910e-01,  6.86266795e-02,  3.05651844e-01,\n",
      "       -2.16594994e-01,  5.65795541e-01,  1.74592603e-02, -5.89829311e-02,\n",
      "        5.62923193e-01,  2.47360002e-02,  2.44581625e-02,  4.49206829e-02,\n",
      "        8.11969563e-02,  2.82718986e-01, -1.35144919e-01, -6.90971911e-01,\n",
      "       -1.27986088e-01, -1.12960991e-02,  9.52371210e-02,  2.13680327e-01,\n",
      "        1.03951067e-01,  7.39899129e-02,  5.67440689e-02,  3.58561188e-01,\n",
      "        1.37203440e-01, -4.37407009e-02, -5.72243668e-02,  1.78861730e-02,\n",
      "        3.35348956e-02,  7.68687204e-02, -5.97735085e-02, -8.46636146e-02,\n",
      "        1.63956150e-01,  8.33482742e-01,  2.23168172e-04, -3.74817103e-03,\n",
      "       -1.00922752e-02,  3.02242823e-02, -1.12265088e-01,  1.46627665e-01,\n",
      "       -5.32912426e-02,  1.51208982e-01,  6.17082268e-02, -3.58447507e-02,\n",
      "        8.59157145e-02,  4.74083796e-03, -1.35819942e-01, -5.24044074e-02,\n",
      "       -2.54761696e-01, -6.05813503e-01,  2.21009851e-01,  1.27605021e-01],\n",
      "      dtype=float32), 'is': array([ 0.35770547, -0.5569449 , -0.2561212 , -0.37850234, -0.57300067,\n",
      "       -0.15021642,  0.34001973,  0.34188583, -0.41179216, -0.15084566,\n",
      "       -0.24684882,  0.39713407,  0.64963037,  0.249207  , -0.37189475,\n",
      "        0.3290677 ,  0.30535507, -0.17848061,  0.30564725, -0.23498973,\n",
      "       -0.5430942 , -0.05189427,  0.10426214, -0.2962366 , -0.20545454,\n",
      "       -0.0187058 ,  0.22258455,  0.7580417 , -0.47481865,  0.34351385,\n",
      "        0.0725643 , -0.5078874 ,  0.40550286,  0.28052095,  0.32529274,\n",
      "       -0.3099019 , -0.6205474 , -0.2805429 ,  0.21359889,  0.00854517,\n",
      "        0.18001091, -0.34665525, -0.08686992, -0.02450782,  0.63541263,\n",
      "        0.1204956 , -0.1990663 , -0.70119834, -0.10718763, -0.14458787,\n",
      "        0.43794167,  0.00951875, -0.46315587, -0.08154805, -0.33761013,\n",
      "       -0.5274414 ,  0.16252537, -0.35666814,  0.02595644, -0.22376403,\n",
      "       -0.45181665,  0.30014974, -0.13643439, -0.18385012,  0.45507738,\n",
      "       -0.21648405,  0.35638165,  0.39896807,  0.55617315, -0.2976325 ,\n",
      "        0.39639565, -0.05910618,  0.12802732, -0.43835735,  0.3290375 ,\n",
      "       -0.2639309 ,  0.34230092,  0.13763793,  0.37750757,  0.13262245,\n",
      "        0.49542367,  0.01957818, -0.17163174,  0.21028057,  0.23196553,\n",
      "        0.2736572 , -0.5918126 ,  0.6408794 , -0.43254066, -0.33653635,\n",
      "       -0.21049766,  0.09745386,  0.15164304, -0.2723453 ,  0.0212054 ,\n",
      "       -0.31022286, -0.2654186 , -0.2825584 ,  0.3679876 ,  0.31870264],\n",
      "      dtype=float32), 'kept': array([-1.13925792e-01, -9.15796757e-02, -1.31464347e-01,  7.48715252e-02,\n",
      "        6.73953742e-02,  1.77667350e-01, -8.04262534e-02,  6.18165694e-02,\n",
      "        8.82090703e-02, -1.37800753e-01,  7.74870766e-03, -7.70831108e-02,\n",
      "       -1.03139132e-02, -1.36157051e-01,  2.61739604e-02, -8.02374035e-02,\n",
      "       -6.00564480e-02, -2.24042684e-02, -5.49502894e-02, -9.13884863e-02,\n",
      "        1.33566663e-01, -6.89881593e-02, -8.16093013e-03,  1.01293631e-01,\n",
      "        1.19404100e-01,  3.54489863e-01,  6.00672849e-02, -1.17327005e-01,\n",
      "        1.34580702e-01,  8.67428537e-03,  1.03262693e-01,  1.09007880e-01,\n",
      "       -1.77822068e-01,  3.14034782e-02, -1.41848534e-01, -1.45919874e-01,\n",
      "        3.47211398e-02,  1.57562762e-01,  1.88526325e-03,  1.00728162e-01,\n",
      "       -1.84938639e-01, -3.17273848e-03, -1.61482394e-02,  9.32477415e-02,\n",
      "       -1.01625286e-01,  4.80016954e-02,  2.31208786e-01,  1.23843789e-01,\n",
      "        3.72866243e-01,  1.99729457e-01,  9.12787095e-02,  8.66761059e-02,\n",
      "       -7.02375546e-03,  5.13922125e-02,  6.64900914e-02, -2.73153707e-02,\n",
      "        1.35591432e-01,  3.68662655e-01,  6.59494475e-03, -5.96531779e-02,\n",
      "        4.36178073e-02,  2.98127197e-02, -1.23303026e-01, -2.79904250e-02,\n",
      "       -5.36015332e-02,  3.78228202e-02, -1.32259130e-01, -8.92850459e-02,\n",
      "       -1.04040675e-01,  2.47501001e-01, -1.25242010e-01, -6.53832685e-03,\n",
      "       -3.27182002e-03,  8.17347392e-02, -3.06717992e-01,  8.00070912e-02,\n",
      "       -1.44905403e-01, -1.21561877e-01,  2.97725856e-01, -3.64151716e-01,\n",
      "       -3.80571932e-02,  3.14711392e-01,  4.78817485e-02, -2.69521475e-02,\n",
      "        1.18679665e-02, -7.65185505e-02, -1.46370018e-02, -1.92387983e-01,\n",
      "        1.04629964e-01,  2.62195244e-05, -1.21603251e-01, -8.09115618e-02,\n",
      "       -1.57188680e-02,  1.85251564e-01,  1.60526633e-01,  1.22661904e-01,\n",
      "       -1.32356435e-02, -1.79563612e-01,  7.12714717e-02, -6.63326979e-02],\n",
      "      dtype=float32), 'person': array([-0.07542584,  0.17969075,  0.5380581 , -0.19508284,  0.3506675 ,\n",
      "        0.29517403, -0.32013407,  0.05700014,  0.43639928,  0.26455212,\n",
      "        0.3727809 , -0.30202252, -0.21966662,  0.1254227 ,  0.1914424 ,\n",
      "       -0.16192861, -0.16754073,  0.05885526, -0.4172279 ,  0.36551723,\n",
      "        0.01836739,  0.47683933, -0.4464246 ,  0.21154642,  0.01725176,\n",
      "       -0.5171776 , -0.22611941, -0.18768273,  0.3311418 , -0.24927905,\n",
      "       -0.17875288,  0.01452216,  0.01437935,  0.08561561, -0.0236277 ,\n",
      "        0.24460326,  0.2934585 ,  0.43826833, -0.58683926, -0.30668527,\n",
      "        0.2864201 ,  0.33689395,  0.5979372 , -0.30485886, -0.1544537 ,\n",
      "       -0.49594724, -0.6338795 ,  0.30809504,  0.0079052 ,  0.5977092 ,\n",
      "       -0.35111374, -0.50778985,  0.45825043, -0.04047301,  0.41172147,\n",
      "        0.41941142, -0.31372276, -0.12107968,  0.65951204,  0.54436314,\n",
      "        0.16230355, -0.23075926,  0.5151567 ,  0.08373707, -0.07141242,\n",
      "        0.22124168, -0.15240623, -0.2836299 , -0.20576179,  0.1692364 ,\n",
      "       -0.3103241 , -0.19341752, -0.21155453,  0.30029675, -0.51062423,\n",
      "        0.3529851 , -0.24852766,  0.32671717, -0.20788495,  0.24441625,\n",
      "        0.00989797, -0.2712544 ,  0.28386253, -0.41111186, -0.40856636,\n",
      "       -0.24620885,  0.28803393, -0.18671605,  0.08209146,  0.09025792,\n",
      "        0.3986389 , -0.13813677, -0.3884766 , -0.05718214, -0.23924518,\n",
      "        0.28953755,  0.50379163,  0.14990145, -0.3598184 , -0.32232597],\n",
      "      dtype=float32), 'the': array([ 0.19325925,  0.07732715,  0.11149048, -0.04461423, -0.11523762,\n",
      "        0.02425102,  0.36017057, -0.13794455, -0.00304159, -0.06752147,\n",
      "       -0.01697796,  0.07902864,  0.15707558, -0.29052415, -0.42580435,\n",
      "        0.2672446 ,  0.28578442, -0.17637922, -0.01400653, -0.06077358,\n",
      "        0.05592363, -0.31558603,  0.07599141, -0.32412857,  0.16439508,\n",
      "       -0.08793644,  0.4276561 ,  0.17118369, -0.20066997,  0.09169731,\n",
      "        0.20010544, -0.11942606, -0.01617894, -0.09066088,  0.12535588,\n",
      "       -0.13689044, -0.12370827,  0.01199239,  0.00114602,  0.17537099,\n",
      "       -0.09256282,  0.11962704,  0.12271228, -0.24052827,  0.11731804,\n",
      "       -0.03733522,  0.18768422, -0.13863093,  0.06793863,  0.10047983,\n",
      "        0.44849706,  0.03653632, -0.14709164, -0.08232468,  0.11505314,\n",
      "       -0.03239842,  0.0061945 ,  0.1191517 ,  0.04508771,  0.06521302,\n",
      "       -0.16116093, -0.03171896, -0.32757396, -0.21357279,  0.081263  ,\n",
      "        0.17225601, -0.20830716,  0.08909687, -0.05808088, -0.02973119,\n",
      "       -0.14464602, -0.09840681,  0.312471  ,  0.14842924, -0.01729234,\n",
      "       -0.09311455, -0.05054629, -0.07916254, -0.05159305, -0.19454674,\n",
      "        0.07536669,  0.23021536,  0.20246878, -0.14215946, -0.1151351 ,\n",
      "       -0.00108364,  0.00403718,  0.12320013, -0.02974937, -0.15827146,\n",
      "       -0.2808847 ,  0.05208771,  0.15146947,  0.05482871,  0.01528081,\n",
      "        0.2510682 ,  0.14459711,  0.02652371, -0.16806787, -0.10803952],\n",
      "      dtype=float32), 'he': array([ 2.26573363e-01,  9.63052586e-02, -6.54204249e-01, -1.97892532e-01,\n",
      "       -4.38258588e-01, -2.28004739e-01,  2.70793378e-01, -8.65586922e-02,\n",
      "       -4.84159663e-02, -3.65073562e-01,  4.17309515e-02, -8.22850913e-02,\n",
      "       -2.03508716e-02, -5.16330935e-02,  8.50379467e-02,  2.50964731e-01,\n",
      "        1.18385270e-01,  2.60809422e-01,  5.30500710e-01, -1.89793333e-01,\n",
      "        3.27778310e-02, -5.77398613e-02,  3.24156195e-01, -2.79594064e-01,\n",
      "       -2.63192445e-01, -2.67830431e-01,  3.26996267e-01, -5.82230277e-02,\n",
      "       -3.82802069e-01,  3.29093933e-01,  3.33870471e-01, -2.24905148e-01,\n",
      "       -1.11666106e-01,  2.96102494e-01,  2.23501008e-02,  1.53962433e-01,\n",
      "        4.41005826e-02,  1.08994013e-02,  2.02996671e-01, -2.77721316e-01,\n",
      "        1.84190825e-01, -2.95178056e-01, -4.32702035e-01,  2.17157304e-02,\n",
      "       -1.79975592e-02,  4.79840934e-01, -2.17387915e-01,  7.43605988e-03,\n",
      "        9.92171392e-02, -2.00172454e-01, -2.43192181e-01,  2.66681224e-01,\n",
      "        6.36341935e-03, -1.46444932e-01,  9.04680043e-02,  8.08303431e-02,\n",
      "        1.43258139e-01,  1.96768530e-02, -5.88473856e-01, -3.16223502e-01,\n",
      "        1.25081092e-01,  3.76288556e-02, -1.84884846e-01,  9.50111374e-02,\n",
      "       -4.39694375e-02, -3.63030344e-01,  2.15879366e-01,  9.43550607e-04,\n",
      "       -3.60416770e-02,  2.33041674e-01,  3.29445779e-01, -2.61083301e-02,\n",
      "        3.92593741e-01, -6.64454699e-02,  5.76147325e-02, -6.96294606e-01,\n",
      "        8.78959596e-02,  1.03277236e-01, -7.47285709e-02,  2.24783048e-01,\n",
      "       -6.42980449e-05, -3.66999209e-01, -4.09264535e-01,  5.02546668e-01,\n",
      "        4.68422085e-01, -1.28043804e-03, -7.47245401e-02,  3.61740083e-01,\n",
      "        7.15367645e-02, -2.90782541e-01,  1.00878298e-01,  2.67944664e-01,\n",
      "       -1.85396522e-02,  6.12240173e-02, -2.11938679e-01, -3.62698376e-01,\n",
      "       -3.10814559e-01, -2.61144817e-01,  1.07118018e-01,  4.74569023e-01],\n",
      "      dtype=float32), 'word': array([-0.11350346, -0.07458715,  0.05939323,  0.17562513,  0.26902482,\n",
      "        0.7907093 , -0.15380566,  0.28804228, -0.07023087,  0.12999086,\n",
      "        0.07322222, -0.07130702, -0.06444861,  0.22388436,  0.33545697,\n",
      "       -0.29388008, -0.04899013,  0.1382588 , -0.17054047,  0.21161966,\n",
      "       -0.1970075 ,  0.1653631 , -0.27467322,  0.34624016, -0.16618334,\n",
      "        0.08435053, -0.3389045 , -0.27977434,  0.16775759, -0.33257258,\n",
      "       -0.31056634,  0.05091114, -0.01371205, -0.02691489, -0.31106448,\n",
      "        0.08964814,  0.11285391,  0.03183595, -0.19701953, -0.19256984,\n",
      "        0.10145682,  0.19908385,  0.31865546,  0.28139845, -0.11477951,\n",
      "       -0.16414545, -0.3883448 ,  0.03126311,  0.01007283,  0.3017158 ,\n",
      "       -0.2318326 , -0.3950009 ,  0.23755722,  0.07052768,  0.16369641,\n",
      "        0.17511985,  0.0113898 , -0.21698374,  0.4179034 ,  0.2724175 ,\n",
      "        0.05419108, -0.03342962,  0.05055213,  0.14650677, -0.19433516,\n",
      "        0.05604285,  0.16171798, -0.06681541, -0.06884895,  0.03380045,\n",
      "       -0.29799584,  0.07236463, -0.07620461, -0.07310341, -0.10090476,\n",
      "        0.24313611, -0.12578055,  0.12350579,  0.1830867 , -0.01534152,\n",
      "       -0.02377525, -0.03480959, -0.19162205, -0.1628356 , -0.08675966,\n",
      "       -0.02825608,  0.0447983 , -0.23628412, -0.09012668,  0.17778091,\n",
      "        0.06787589, -0.24132766, -0.08516377, -0.0508333 ,  0.18551649,\n",
      "        0.0207181 ,  0.1425806 ,  0.04246468,  0.00910656,  0.0033677 ],\n",
      "      dtype=float32), 'keeping': array([ 0.07616719, -0.01400804, -0.05847878, -0.12370723, -0.2621943 ,\n",
      "       -0.08118118,  0.03726098, -0.38583988,  0.33461365, -0.32069382,\n",
      "        0.29645082, -0.24380615, -0.31715527, -0.44425726, -0.1075675 ,\n",
      "        0.22583954, -0.04262796, -0.19860636,  0.03069858, -0.16606836,\n",
      "        0.5436911 , -0.09956071, -0.01543435, -0.26406094,  0.0027467 ,\n",
      "       -0.07643696,  0.40200275, -0.20723307, -0.01709499,  0.24451995,\n",
      "        0.31752542,  0.13232471, -0.4849718 ,  0.01928242,  0.16128185,\n",
      "        0.08955529,  0.01014789,  0.35728222, -0.07732931,  0.1230829 ,\n",
      "        0.16430151,  0.0331383 , -0.26207787, -0.08922108, -0.22170378,\n",
      "        0.10007513, -0.03637329,  0.4060429 ,  0.26961488,  0.24058323,\n",
      "        0.05202644,  0.05674678, -0.05558762, -0.23065038,  0.1405624 ,\n",
      "        0.17761514,  0.11266822,  0.32492682,  0.20343468, -0.12226528,\n",
      "       -0.0014652 , -0.31097284, -0.08365158,  0.14058858, -0.10908463,\n",
      "       -0.18658295, -0.11282956, -0.20719908, -0.40302885,  0.18526025,\n",
      "       -0.05574194, -0.01690153,  0.2952007 ,  0.18985286, -0.449845  ,\n",
      "       -0.0998615 , -0.36140302, -0.17672336, -0.43548003,  0.20341687,\n",
      "       -0.41301003,  0.15297987,  0.20081499, -0.02733116,  0.03444999,\n",
      "       -0.10143783,  0.09931978, -0.02810304,  0.45779637, -0.144412  ,\n",
      "        0.29741603,  0.2653792 , -0.13391249,  0.24876727, -0.21664844,\n",
      "        0.26345292, -0.15990478,  0.31604576, -0.36200526,  0.00850699],\n",
      "      dtype=float32), 'good': array([-0.10186158, -0.1146246 , -0.06936637, -0.01684042,  0.20912318,\n",
      "       -0.17301457, -0.09151918,  0.6150558 , -0.12003341, -0.20699054,\n",
      "        0.17600454,  0.29519352,  0.40106088,  0.50062877, -0.12077158,\n",
      "       -0.16516908, -0.29830384,  0.15920582, -0.08768465,  0.13765816,\n",
      "       -0.50947744, -0.02411717, -0.11230953,  0.06751878,  0.05638883,\n",
      "        0.23597938,  0.02005103,  0.17293465,  0.11044007, -0.18824899,\n",
      "       -0.05889958,  0.10546592,  0.41068915,  0.37799165, -0.20578794,\n",
      "        0.28614694, -0.08394085, -0.10630261, -0.04068399, -0.03511377,\n",
      "        0.11412547, -0.08995513, -0.03971358,  0.35286877,  0.43055913,\n",
      "       -0.22284773,  0.10926384,  0.03026509,  0.4241574 , -0.2788072 ,\n",
      "       -0.2494353 ,  0.05368007,  0.22633415,  0.3886417 , -0.07126587,\n",
      "        0.11794135,  0.28971085, -0.09254915,  0.0943266 ,  0.11520612,\n",
      "        0.21446995,  0.29136923,  0.2856004 , -0.44412068, -0.06943171,\n",
      "       -0.16274355,  0.14786844,  0.19561788,  0.31589648,  0.20273757,\n",
      "       -0.02386904,  0.40106222,  0.04462301, -0.5159855 , -0.20739616,\n",
      "       -0.07001751,  0.06296959,  0.14296664,  0.2509154 , -0.14075397,\n",
      "        0.37603942,  0.09215661,  0.21369445, -0.00466848, -0.25553194,\n",
      "       -0.09826495, -0.36393693,  0.25382632, -0.22945306,  0.14717895,\n",
      "        0.07692676, -0.12551872,  0.05321806, -0.28117067, -0.25867417,\n",
      "       -0.01407385,  0.05063727, -0.35471576,  0.33911505,  0.07444176],\n",
      "      dtype=float32), 'knew': array([ 0.1280212 , -0.16904268, -0.15525189, -0.22684707, -0.00492749,\n",
      "       -0.32428917,  0.12854303, -0.1154443 , -0.13824493, -0.31062165,\n",
      "        0.11231452,  0.19503069,  0.01794386, -0.04958885, -0.24448532,\n",
      "        0.17485066,  0.22432563, -0.13698044,  0.01770472, -0.13997526,\n",
      "       -0.12091056, -0.12138023,  0.24013789, -0.12274025, -0.2623276 ,\n",
      "       -0.17164294,  0.3029093 ,  0.13205396, -0.16763   ,  0.2867804 ,\n",
      "        0.14231382, -0.03496812,  0.14095998,  0.64092857,  0.09814242,\n",
      "       -0.00171979, -0.22254327, -0.07058351,  0.0284926 ,  0.04169108,\n",
      "        0.2591072 , -0.20337926, -0.19028106, -0.03160441,  0.22828266,\n",
      "        0.24106856, -0.10943925, -0.05828238,  0.5742287 ,  0.20686904,\n",
      "       -0.0066432 ,  0.20339212, -0.25379768, -0.09853701,  0.01101981,\n",
      "        0.10950541,  0.20914753, -0.03067796, -0.14131409, -0.12650198,\n",
      "       -0.04059434,  0.01284163,  0.0208316 , -0.3921611 ,  0.20972751,\n",
      "       -0.2533185 ,  0.2561537 ,  0.10519478,  0.06049882,  0.16497771,\n",
      "        0.20730908,  0.41769388,  0.10172011, -0.12329139,  0.07753337,\n",
      "       -0.22667156,  0.04861841, -0.0090034 ,  0.04401312,  0.23941842,\n",
      "        0.1064864 , -0.04954697, -0.17709911,  0.3552892 ,  0.10485701,\n",
      "        0.12976342, -0.15867485,  0.10470226, -0.11102132, -0.1915614 ,\n",
      "        0.13058539,  0.14730081,  0.07907301, -0.21866423, -0.2350056 ,\n",
      "       -0.15486346, -0.17048794, -0.51081973,  0.00681769,  0.26749384],\n",
      "      dtype=float32), '!': array([-0.5758458 ,  0.5110901 ,  0.53908926,  0.58492744,  0.43513978,\n",
      "       -0.58273304, -0.43258673, -0.31595564,  0.11893997,  0.5637028 ,\n",
      "        0.22826508, -0.13771528, -0.2753252 ,  0.04032516,  0.28032112,\n",
      "       -0.589805  , -0.59683496,  0.5988696 , -0.26691708,  0.46867883,\n",
      "        0.40012422,  0.48523447, -0.54513747,  0.5308323 ,  0.3856494 ,\n",
      "       -0.49069276, -0.54378194, -0.17781432,  0.37933004, -0.553142  ,\n",
      "       -0.5791251 ,  0.25831512,  0.20693302, -0.3178751 , -0.5221882 ,\n",
      "        0.64092946,  0.3695378 ,  0.24869213, -0.42594332, -0.5705072 ,\n",
      "       -0.40621606,  0.50364274,  0.26846287, -0.53310716, -0.15412961,\n",
      "       -0.4224363 ,  0.19034399,  0.15814425, -0.40320152, -0.36620852,\n",
      "       -0.6402787 , -0.51267606,  0.3926538 , -0.07650612,  0.410797  ,\n",
      "        0.2948827 , -0.4052262 , -0.20826577,  0.07632128,  0.4060555 ,\n",
      "        0.48968533, -0.11453205,  0.60893273,  0.19621187, -0.5101824 ,\n",
      "        0.5580123 , -0.2830597 , -0.31324703,  0.00924123,  0.1775347 ,\n",
      "       -0.17472267,  0.23642433, -0.26169133,  0.25387293,  0.15905736,\n",
      "        0.3772339 , -0.17284003,  0.5264567 , -0.3021143 ,  0.37932622,\n",
      "       -0.41443634, -0.5669431 ,  0.52396375, -0.3746031 , -0.2971168 ,\n",
      "       -0.5119798 ,  0.23067556, -0.00345879,  0.3921187 ,  0.47585177,\n",
      "        0.36014694, -0.5489919 , -0.55997854, -0.06512329, -0.58589804,\n",
      "        0.25585094,  0.4309672 , -0.10151856, -0.0163605 , -0.35631505],\n",
      "      dtype=float32), 'but': array([-0.04698271,  0.36570147, -0.34035   ,  0.03199894, -0.1485311 ,\n",
      "        0.20415002,  0.14393173, -0.28863114,  0.5162814 , -0.01338281,\n",
      "       -0.35398832, -0.4837712 , -0.21261556, -0.40880007,  0.35696122,\n",
      "        0.08014906,  0.03916419,  0.15895514,  0.13226618, -0.06024435,\n",
      "        0.32924578, -0.05973101,  0.10989752,  0.00432782, -0.02300058,\n",
      "       -0.19488294, -0.3394513 , -0.49001947, -0.08354365,  0.17118634,\n",
      "        0.09810293,  0.15133002, -0.14917025,  0.10784417, -0.01291713,\n",
      "       -0.09185681,  0.4616336 ,  0.3795448 , -0.06455721,  0.17549002,\n",
      "       -0.07906269, -0.07277278, -0.15812738,  0.31120402, -0.29914337,\n",
      "        0.25583833,  0.05770826,  0.11703341,  0.05933446,  0.13524236,\n",
      "       -0.22697754,  0.2060011 ,  0.2866447 , -0.02486285,  0.36010477,\n",
      "        0.3410004 , -0.4026317 ,  0.42441204, -0.58586913, -0.11625902,\n",
      "        0.10777951, -0.26671684,  0.02834662,  0.22981183, -0.14836583,\n",
      "        0.00740753,  0.005752  , -0.48842916, -0.10589506,  0.01857123,\n",
      "       -0.17238708, -0.25544304, -0.08808098,  0.25536558,  0.17217281,\n",
      "       -0.20091733,  0.22586115, -0.19774145,  0.09448326,  0.00103143,\n",
      "       -0.26144275, -0.36641493, -0.04882723,  0.00825377,  0.24924394,\n",
      "       -0.35285217,  0.44238648, -0.069057  ,  0.27807662, -0.02372007,\n",
      "       -0.14343707,  0.015993  , -0.35699904,  0.35779998, -0.1365751 ,\n",
      "       -0.48486724, -0.04610492, -0.12206888,  0.03702391, -0.04879542],\n",
      "      dtype=float32), 'and': array([ 0.20762767, -0.26078725, -0.28237128, -0.20097558, -0.20592763,\n",
      "        0.06196322,  0.33036405, -0.1231808 , -0.10465889, -0.3444836 ,\n",
      "        0.25481826,  0.07364804,  0.00899794, -0.10949319, -0.29039043,\n",
      "        0.24328358,  0.31636   , -0.27820694,  0.34883583, -0.25606292,\n",
      "       -0.01663982, -0.13292998,  0.21820828, -0.2349044 , -0.15597658,\n",
      "        0.03707788,  0.27340338,  0.03311953, -0.32627055,  0.4467888 ,\n",
      "        0.26089385, -0.17285421,  0.00722369,  0.32814094,  0.07959569,\n",
      "       -0.26320222, -0.17487974,  0.07196663,  0.12607151,  0.28609103,\n",
      "        0.21673769, -0.35917443, -0.31933498,  0.2555798 ,  0.1545609 ,\n",
      "        0.20983076, -0.12723881, -0.13380863,  0.39893326,  0.14114633,\n",
      "        0.14534567,  0.30361763, -0.3135458 , -0.07064408,  0.0754693 ,\n",
      "        0.06921335,  0.13326067,  0.11787565, -0.3034932 , -0.29232478,\n",
      "       -0.11457654,  0.01592586, -0.00312984, -0.27414337,  0.18188521,\n",
      "       -0.2872515 ,  0.39768052,  0.0601463 ,  0.06068281, -0.23547591,\n",
      "        0.20316686,  0.14886647,  0.18859595, -0.01088999, -0.11364404,\n",
      "       -0.22905198,  0.00960835, -0.19105391,  0.14462675,  0.06370349,\n",
      "        0.00262634,  0.26703873, -0.2716186 ,  0.23625247,  0.10330939,\n",
      "       -0.02599422, -0.06933463,  0.28793746, -0.02955407, -0.17634596,\n",
      "        0.05894299,  0.22310653,  0.05977323, -0.16150723, -0.05125933,\n",
      "       -0.41292214, -0.4537243 , -0.21294874,  0.03896017,  0.38472167],\n",
      "      dtype=float32), 'such': array([-0.14318697,  0.16051503, -0.17290334, -0.10441197, -0.07201194,\n",
      "       -0.02809544, -0.17649314, -0.17111784,  0.08081055, -0.106895  ,\n",
      "       -0.07061436, -0.06265408, -0.34810212, -0.17844033,  0.10307637,\n",
      "        0.01331761, -0.09637555,  0.1537748 ,  0.00136569,  0.08302   ,\n",
      "        0.29240143, -0.0672265 ,  0.01958248, -0.13252358, -0.15978152,\n",
      "       -0.09243168, -0.21832848, -0.27259317,  0.01401016,  0.09741344,\n",
      "       -0.11070015, -0.03912959, -0.32623366, -0.08626661,  0.07021736,\n",
      "       -0.03345274,  0.11380688, -0.10809419,  0.18063203,  0.02125409,\n",
      "        0.1157486 ,  0.07408221, -0.09038044, -0.09996917, -0.28392136,\n",
      "        0.12923497, -0.09388808,  0.35195604, -0.23333773,  0.15858397,\n",
      "        0.16286737,  0.14638804, -0.09212294, -0.03872973,  0.01680025,\n",
      "       -0.00768837, -0.04247456, -0.00645723,  0.14557835,  0.06029842,\n",
      "       -0.00753361, -0.24441595, -0.10833746,  0.23423783,  0.16600946,\n",
      "       -0.12808722,  0.06051546, -0.2016247 , -0.10324825,  0.11630857,\n",
      "        0.09344655,  0.11016521, -0.11154602,  0.02911415, -0.02308248,\n",
      "        0.17287384, -0.09835373,  0.02414551, -0.23822907,  0.11273721,\n",
      "       -0.3255817 , -0.03869677, -0.01478705, -0.03504712, -0.03034986,\n",
      "        0.0873296 ,  0.02939355, -0.06596282,  0.27704677,  0.0193614 ,\n",
      "        0.13128321, -0.04630921, -0.07995384,  0.01890178, -0.28118604,\n",
      "        0.1254378 , -0.12278701,  0.0727839 , -0.23324904, -0.06734564],\n",
      "      dtype=float32), 'to': array([ 0.16140631, -0.4533681 , -0.36792466, -0.20818385,  0.04206529,\n",
      "        0.28955787,  0.13437887,  0.17266825, -0.2218447 , -0.3728125 ,\n",
      "       -0.1487207 ,  0.23409028,  0.19026645, -0.05081988, -0.15858641,\n",
      "        0.18937482,  0.09751642, -0.2709529 ,  0.14126736, -0.2205699 ,\n",
      "       -0.1847427 , -0.166757  ,  0.28689533, -0.19602738, -0.14862138,\n",
      "        0.32704487,  0.30153424,  0.11494607, -0.13890666,  0.12747577,\n",
      "        0.24611482, -0.07821561, -0.0386785 ,  0.07100222, -0.02184682,\n",
      "       -0.16774333, -0.16856967, -0.13219135,  0.19837119,  0.317881  ,\n",
      "        0.01924725, -0.28621277, -0.23735537,  0.29354292,  0.21698023,\n",
      "        0.28080678,  0.0096255 , -0.17699224,  0.17344332,  0.17699802,\n",
      "        0.11317115,  0.26128668, -0.15814379, -0.03584518, -0.18754524,\n",
      "       -0.25713563,  0.26547965,  0.02123683, -0.41024935, -0.32700655,\n",
      "       -0.16898918, -0.05620588, -0.25612482, -0.13205141,  0.25275367,\n",
      "       -0.2456735 ,  0.1317799 ,  0.13144922,  0.2897145 , -0.17021652,\n",
      "        0.20686026, -0.19323914,  0.07220504, -0.24027187,  0.18763839,\n",
      "       -0.01602138,  0.09631597, -0.32231134,  0.35994917, -0.07849361,\n",
      "        0.25208968,  0.19933824, -0.27475688,  0.4494409 ,  0.16622216,\n",
      "        0.04075714, -0.3507195 ,  0.080987  , -0.1808807 , -0.20258477,\n",
      "       -0.24644671,  0.12273636,  0.13186176, -0.06939079,  0.27646858,\n",
      "       -0.37735212, -0.33566394, -0.10622393,  0.28845263,  0.28865188],\n",
      "      dtype=float32), 'himself': array([ 0.21181163, -0.36470416, -0.16841832, -0.39517498, -0.24204825,\n",
      "        0.21135399,  0.30795598,  0.12965763, -0.18645625, -0.35107413,\n",
      "       -0.22255602,  0.09759284,  0.08356573,  0.03891522, -0.16938424,\n",
      "        0.24606706,  0.09347955, -0.3857339 ,  0.35532495, -0.20838799,\n",
      "       -0.21870187, -0.22250982,  0.12469973, -0.37844282, -0.22486123,\n",
      "        0.00781279,  0.19283374,  0.05858382, -0.19158193,  0.4564    ,\n",
      "        0.25901192, -0.24528931,  0.18889667,  0.16916946,  0.45194528,\n",
      "       -0.21462296, -0.29714796, -0.19775951,  0.21675237,  0.2823608 ,\n",
      "        0.34367523, -0.27703005, -0.38201255,  0.3427133 ,  0.3270927 ,\n",
      "        0.43828017, -0.3117004 , -0.14597149, -0.0945424 ,  0.18287154,\n",
      "        0.26611486,  0.43998453, -0.26895475, -0.16069046, -0.10209576,\n",
      "       -0.34903392,  0.10969373, -0.12976399, -0.261523  , -0.33487648,\n",
      "       -0.27683216, -0.04469075,  0.03386898,  0.14359611,  0.39846212,\n",
      "       -0.25559995,  0.24333891,  0.04254792,  0.14204225, -0.49469745,\n",
      "        0.30928746, -0.03934401,  0.18829785, -0.1536981 ,  0.41380453,\n",
      "       -0.07369287,  0.18398039, -0.26542512,  0.13630201,  0.18585569,\n",
      "        0.16941445, -0.13529107, -0.20036879,  0.2915858 ,  0.1458815 ,\n",
      "        0.20945662, -0.2116888 ,  0.277369  , -0.14575885, -0.13707024,\n",
      "       -0.08735353,  0.3147342 ,  0.04538033, -0.30227628,  0.08872719,\n",
      "       -0.3187281 , -0.3374729 , -0.05421277,  0.04407558,  0.3213468 ],\n",
      "      dtype=float32), 'was': array([ 0.23543158, -0.38865554, -0.323978  , -0.18528672, -0.31214026,\n",
      "        0.06420493,  0.12283744,  0.01529582, -0.4568188 , -0.1758187 ,\n",
      "       -0.01963468,  0.27211392,  0.16704157,  0.18985239, -0.24804142,\n",
      "        0.15038408,  0.18658221, -0.06789874,  0.30478656, -0.10017015,\n",
      "       -0.26889774, -0.05473403,  0.25265568, -0.07627913, -0.1980125 ,\n",
      "        0.25598803,  0.12023394,  0.13475391, -0.2084117 ,  0.19339846,\n",
      "        0.04186665, -0.22072496,  0.10923895,  0.29608223,  0.01014127,\n",
      "       -0.0652518 , -0.22512153, -0.3652186 ,  0.29325876, -0.02918212,\n",
      "        0.33523488, -0.22643507, -0.3941492 ,  0.10569739,  0.08159679,\n",
      "        0.32758465, -0.35089728, -0.27899528,  0.14534754, -0.07311644,\n",
      "       -0.03163811,  0.15717812, -0.08721874,  0.14870152, -0.22168188,\n",
      "       -0.21707295,  0.5343128 , -0.39407492, -0.3486759 , -0.14379984,\n",
      "       -0.05483415,  0.26609212, -0.38163763, -0.43474314,  0.0765585 ,\n",
      "       -0.21895319,  0.33039138,  0.344024  ,  0.2758008 , -0.07995455,\n",
      "        0.49248993,  0.02249413,  0.12604082, -0.33707947,  0.19677307,\n",
      "       -0.25850484,  0.27939802, -0.10187431,  0.22268273,  0.38442737,\n",
      "        0.31251267,  0.3324902 , -0.3025355 ,  0.3903022 ,  0.33235368,\n",
      "        0.05032389, -0.3357727 ,  0.26925611, -0.15566747, -0.11692166,\n",
      "       -0.03099425,  0.14443679,  0.08373867, -0.21499686,  0.07450569,\n",
      "       -0.3939329 , -0.29354632, -0.41294336,  0.3634306 ,  0.31831577],\n",
      "      dtype=float32), 'driving': array([ 1.14621393e-01,  2.86748377e-03, -2.54736450e-02, -1.30022526e-01,\n",
      "        1.50151346e-02, -2.65548557e-01,  2.32265145e-02, -3.73517752e-01,\n",
      "       -2.98453476e-02, -1.85226351e-01,  9.07818228e-02,  2.01611090e-02,\n",
      "       -7.06906915e-02, -2.00271532e-01,  2.36307122e-02,  1.24971094e-02,\n",
      "       -4.21438646e-03,  1.48905953e-02,  4.05595563e-02, -5.50587215e-02,\n",
      "        3.58077958e-02,  5.75468992e-04,  1.46264791e-01, -8.03641528e-02,\n",
      "       -4.15973365e-01, -1.00873396e-01,  9.05113146e-02,  1.94660779e-02,\n",
      "       -2.03519147e-02,  1.95691496e-01,  1.72706813e-01, -6.38280362e-02,\n",
      "       -9.77033600e-02, -9.98715777e-03,  2.81701349e-02,  2.35239975e-02,\n",
      "       -9.86120291e-03, -4.03157733e-02,  9.65766981e-03, -3.74470428e-02,\n",
      "        4.13064271e-01, -2.18640402e-01, -3.53344202e-01, -6.33980520e-03,\n",
      "        3.67657058e-02,  2.53401965e-01, -4.56969678e-01,  3.82985249e-02,\n",
      "        4.72762734e-02,  1.98297173e-01, -8.69450271e-02,  2.91595310e-01,\n",
      "        1.39444205e-03, -3.92281502e-01, -7.34628215e-02, -1.60441846e-02,\n",
      "       -5.56776412e-02, -7.60579156e-03, -1.50490031e-01, -2.53070444e-01,\n",
      "        8.96874219e-02, -4.90334183e-02,  1.75139606e-01,  1.36639044e-01,\n",
      "       -6.78033277e-04, -2.78358281e-01,  1.99755743e-01, -2.49811187e-02,\n",
      "        3.06543056e-02,  1.62040852e-02,  2.26994306e-01, -6.63927048e-02,\n",
      "        8.69223773e-02,  1.04709323e-02,  1.74091645e-02, -6.42254427e-02,\n",
      "        4.27790359e-02, -1.74226426e-02, -2.23321304e-01,  7.62284636e-01,\n",
      "        1.36284661e-02, -3.03711772e-01, -4.69976366e-02,  2.26583570e-01,\n",
      "        1.02679595e-01,  4.09559682e-02,  7.55502656e-02,  9.69171822e-02,\n",
      "        7.00302124e-02, -2.42135763e-01, -4.84811813e-02,  2.59062231e-01,\n",
      "       -1.46951815e-02,  1.32470354e-02, -5.01694500e-01, -1.17661305e-01,\n",
      "       -1.62294701e-01,  1.54675931e-01, -1.20683461e-01,  1.41211405e-01],\n",
      "      dtype=float32), 'crazy': array([ 0.15989749, -0.3819939 , -0.48347023, -0.15959072,  0.08563896,\n",
      "        0.4485431 ,  0.3881616 ,  0.22408003,  0.05511652, -0.5161619 ,\n",
      "       -0.33878115,  0.0010383 ,  0.11158377, -0.02497468, -0.3190857 ,\n",
      "        0.02639964,  0.22264884, -0.453097  ,  0.23545292, -0.5033537 ,\n",
      "       -0.20138802, -0.40549085,  0.3730966 , -0.20857356, -0.20910546,\n",
      "        0.59717625,  0.32360464, -0.01363403, -0.1841588 ,  0.18627878,\n",
      "        0.5055569 , -0.00077437, -0.09582477, -0.04796403,  0.18044762,\n",
      "       -0.61621666, -0.1680186 , -0.03125257,  0.14593506,  0.6263804 ,\n",
      "       -0.12304444, -0.44031245, -0.24187365,  0.6131324 ,  0.19915092,\n",
      "        0.30994627,  0.04126123,  0.01975062,  0.18550156,  0.41799524,\n",
      "        0.49162772,  0.4998134 , -0.18420787,  0.01736671, -0.21931522,\n",
      "       -0.23025331,  0.19141799,  0.01660909, -0.5115098 , -0.44816348,\n",
      "       -0.29166016, -0.06609771, -0.24548796, -0.09265391,  0.46332332,\n",
      "       -0.28501406,  0.24082941, -0.01673867,  0.03252621, -0.4049503 ,\n",
      "        0.00206723, -0.33676922,  0.06181375, -0.12596464, -0.03478713,\n",
      "        0.05252614,  0.07051231, -0.5160177 ,  0.34645376, -0.11260813,\n",
      "       -0.00263301,  0.40367338, -0.10714053,  0.16423413,  0.0837015 ,\n",
      "        0.15574324, -0.18961318, -0.08727285, -0.03640303, -0.457257  ,\n",
      "       -0.3541168 ,  0.27086246,  0.18124996,  0.0205627 ,  0.36168924,\n",
      "       -0.17429654, -0.31101507, -0.01419015,  0.2003566 ,  0.22298886],\n",
      "      dtype=float32), 'went': array([ 0.06316201, -0.21919581, -0.27484274,  0.10732462, -0.14501925,\n",
      "        0.10511319,  0.14675055,  0.05740702,  0.04171891, -0.11861385,\n",
      "       -0.46946964, -0.33798984, -0.15055442, -0.08948222,  0.57315826,\n",
      "       -0.02761962,  0.13491261,  0.06747688,  0.47146094,  0.07585747,\n",
      "        0.04920173, -0.20978636,  0.18527827,  0.1493827 , -0.25367206,\n",
      "       -0.41046217, -0.01362118, -0.10843267, -0.42019328,  0.18465549,\n",
      "       -0.08583164, -0.17246792,  0.0790731 , -0.1980991 , -0.13444155,\n",
      "       -0.06151881,  0.11328104, -0.20646405,  0.48740298, -0.07340687,\n",
      "        0.14677185, -0.01335321, -0.17698261,  0.27486095, -0.10859132,\n",
      "        0.08634211, -0.25592163, -0.27507988, -0.2820729 ,  0.17768076,\n",
      "       -0.28825673,  0.19879414,  0.14471248, -0.44445127,  0.01584888,\n",
      "        0.16496274, -0.23508562, -0.37730783, -0.5394799 , -0.13492571,\n",
      "       -0.05226822, -0.38048744,  0.13817771,  0.5001627 ,  0.02130514,\n",
      "       -0.09989588,  0.06083694, -0.1695392 , -0.127976  , -0.20176807,\n",
      "        0.11162153, -0.26573607, -0.12657744, -0.0171306 ,  0.5064248 ,\n",
      "       -0.23861717,  0.5874645 , -0.1258439 , -0.07068868,  0.2954384 ,\n",
      "       -0.23123066, -0.75468206, -0.5384235 ,  0.3573037 ,  0.57096756,\n",
      "        0.22764221,  0.23094368,  0.02443294, -0.07656303,  0.09500919,\n",
      "       -0.2543725 ,  0.30673546, -0.02232524, -0.10268294,  0.21885386,\n",
      "       -0.27230355,  0.07513743,  0.11124755,  0.39565894,  0.07965151],\n",
      "      dtype=float32), 'up': array([ 0.17521995,  0.14216024, -0.19556926, -0.14764568, -0.32017902,\n",
      "        0.0982545 ,  0.26203075, -0.36245942,  0.1074919 , -0.43221185,\n",
      "       -0.06746665, -0.21948254, -0.165594  , -0.28035876,  0.42908138,\n",
      "        0.28352386,  0.09609547,  0.07938185,  0.37865838, -0.18317978,\n",
      "        0.11765434, -0.03088961,  0.37614587, -0.14849801, -0.25863594,\n",
      "       -0.35687196,  0.44177693, -0.25024235, -0.15784502,  0.37733686,\n",
      "        0.33232996, -0.00714571, -0.03243689,  0.04704249,  0.02784476,\n",
      "        0.11184042,  0.18012328,  0.24287888,  0.06235708,  0.02425064,\n",
      "        0.20416982, -0.2007962 , -0.3437658 ,  0.35828727, -0.09387476,\n",
      "        0.42154863, -0.0400006 ,  0.1391559 ,  0.07070526,  0.2537782 ,\n",
      "       -0.43727332,  0.38383803,  0.14215021, -0.26784185,  0.15863793,\n",
      "        0.3468086 , -0.15112284,  0.05313075, -0.5886478 , -0.294463  ,\n",
      "        0.1636101 , -0.27462766,  0.06507377,  0.34405306, -0.15782489,\n",
      "       -0.3636126 , -0.04470623, -0.28728235, -0.18050379,  0.17447892,\n",
      "        0.11235847, -0.20567134, -0.02990051,  0.05966111, -0.04722581,\n",
      "       -0.27786657,  0.15110023, -0.18140268, -0.10354061,  0.25245368,\n",
      "       -0.21315077, -0.56685644, -0.03873827,  0.4736624 ,  0.46157548,\n",
      "       -0.15397836,  0.20529084,  0.06284779,  0.22431295, -0.22154711,\n",
      "        0.24283081,  0.352078  , -0.13541147, -0.08556643, -0.03356161,\n",
      "       -0.21598387, -0.22455855,  0.01208906, -0.06037716,  0.10447349],\n",
      "      dtype=float32), 'mountain': array([-0.30017537,  0.20835875,  0.094657  ,  0.30264726,  0.18308781,\n",
      "       -0.14867495, -0.23137736, -0.08667664,  0.12100512,  0.27717686,\n",
      "       -0.04980557, -0.20104274, -0.21690504,  0.10227226,  0.3098116 ,\n",
      "       -0.27873597, -0.213751  ,  0.15712558, -0.18145034,  0.26790762,\n",
      "        0.14239492,  0.10449654, -0.337291  ,  0.2714522 ,  0.23830158,\n",
      "       -0.18632002, -0.24107426, -0.18812083,  0.07054094, -0.31662917,\n",
      "       -0.2966678 ,  0.09835199, -0.19314525, -0.20574614, -0.28664437,\n",
      "        0.21102907,  0.23358686,  0.17748713, -0.1608842 , -0.2082229 ,\n",
      "       -0.31141204,  0.24812229,  0.27261657, -0.080408  , -0.19338565,\n",
      "       -0.1541166 ,  0.13760316,  0.1981933 , -0.0883124 , -0.3143358 ,\n",
      "       -0.13078286, -0.38492784,  0.15673004, -0.04409691,  0.14299083,\n",
      "        0.17621791, -0.1055131 , -0.0281769 ,  0.1540162 ,  0.13866964,\n",
      "        0.16711736, -0.01463812,  0.0997721 ,  0.18194678, -0.27899137,\n",
      "        0.23259486, -0.23435013, -0.1744809 , -0.06378376,  0.03388819,\n",
      "       -0.29690215, -0.03955965, -0.16281946,  0.10019766, -0.09345538,\n",
      "        0.21504636, -0.10740071,  0.29892084,  0.04835628, -0.06494275,\n",
      "       -0.18753326, -0.21334973,  0.02354755, -0.14988622, -0.09534682,\n",
      "       -0.27007332,  0.14269571, -0.23562813,  0.22860013,  0.28799555,\n",
      "        0.00855332, -0.2916913 , -0.18222736,  0.16571331, -0.12820658,\n",
      "        0.07083009,  0.1264806 ,  0.15766203, -0.08104384, -0.22658828],\n",
      "      dtype=float32)}\n",
      "[-0.08521013  0.04183844 -0.10600397  0.2816236  -0.08247718 -0.12669058\n",
      "  0.072425   -0.08257344 -0.01619127  0.09917671 -0.03506725 -0.13299862\n",
      " -0.01912012 -0.12782875 -0.05363728  0.03723474 -0.08110777 -0.00920613\n",
      "  0.06212485  0.11659178  0.21787561 -0.195717   -0.18953079 -0.02364411\n",
      "  0.33918756  0.12524231  0.04668527  0.05752857 -0.07381717 -0.09909495\n",
      " -0.11367676 -0.00623834 -0.09746572 -0.27338842 -0.14604788 -0.0295278\n",
      "  0.04513305  0.11273925  0.04954884 -0.0967645  -0.2960083   0.0342954\n",
      "  0.02681018 -0.20370786 -0.01151046  0.04244458  0.16725494  0.0385061\n",
      "  0.00664611 -0.27164692  0.0967521  -0.1557534  -0.08011765  0.0273912\n",
      "  0.07536416 -0.04079482 -0.00194262  0.16298161 -0.05316889  0.03259494\n",
      "  0.05646679 -0.06862847 -0.10371475  0.26638785 -0.04162062  0.25412524\n",
      " -0.1480218  -0.046637   -0.00548266  0.01670582 -0.10419339 -0.09639233\n",
      "  0.09584246  0.04768578 -0.00742401 -0.00079265 -0.08439282  0.09644403\n",
      "  0.16669592 -0.20335813 -0.21905722 -0.09512816 -0.02960181 -0.02814782\n",
      "  0.04662682 -0.06571402 -0.01087444 -0.10929782  0.10362708  0.18473747\n",
      " -0.0638132  -0.07492815 -0.04549425  0.1719871   0.01720976  0.01202237\n",
      "  0.02022345  0.10367794  0.13947898 -0.1181339 ]\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.layers[0].get_weights()[0]\n",
    "\n",
    "words_embeddings = {w:embeddings[idx] for w, idx in vocab_dict.items()}\n",
    "print(words_embeddings)\n",
    "\n",
    "print(words_embeddings['barber'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape[1:] :  (16,)\n",
      "Train on 11 samples, validate on 11 samples\n",
      "Epoch 1/100\n",
      "11/11 [==============================] - 5s 410ms/step - loss: 2.5326 - acc: 0.3693 - val_loss: 2.3959 - val_acc: 0.4886\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 1.7243 - acc: 0.6193 - val_loss: 1.4359 - val_acc: 0.6705\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 1.5537 - acc: 0.6761 - val_loss: 1.1719 - val_acc: 0.6932\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 1.2338 - acc: 0.6932 - val_loss: 0.9919 - val_acc: 0.7045\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 1.0951 - acc: 0.7045 - val_loss: 0.9332 - val_acc: 0.6761\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.8959 - acc: 0.7330 - val_loss: 0.7488 - val_acc: 0.7841\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.7537 - acc: 0.7784 - val_loss: 0.6150 - val_acc: 0.8011\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.7526 - acc: 0.7500 - val_loss: 0.9144 - val_acc: 0.7273\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.7820 - acc: 0.7443 - val_loss: 0.4906 - val_acc: 0.8409\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.4985 - acc: 0.8352 - val_loss: 0.4638 - val_acc: 0.8466\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.5475 - acc: 0.7841 - val_loss: 0.6197 - val_acc: 0.7557\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.4833 - acc: 0.8239 - val_loss: 0.3654 - val_acc: 0.8466\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.3880 - acc: 0.8636 - val_loss: 0.4665 - val_acc: 0.8068\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.4371 - acc: 0.8466 - val_loss: 0.2897 - val_acc: 0.9091\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.3994 - acc: 0.8466 - val_loss: 0.3308 - val_acc: 0.8693\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.3598 - acc: 0.8807 - val_loss: 0.2086 - val_acc: 0.9261\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.1985 - acc: 0.9318 - val_loss: 0.1825 - val_acc: 0.9602\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.1770 - acc: 0.9432 - val_loss: 0.1256 - val_acc: 0.9545\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.1318 - acc: 0.9489 - val_loss: 0.1363 - val_acc: 0.9489\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.1495 - acc: 0.9432 - val_loss: 0.1053 - val_acc: 0.9602\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 0.1216 - acc: 0.9545 - val_loss: 0.3418 - val_acc: 0.8864\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.2741 - acc: 0.9034 - val_loss: 0.1330 - val_acc: 0.9489\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 0.1434 - acc: 0.9545 - val_loss: 0.0904 - val_acc: 0.9773\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 46ms/step - loss: 0.0604 - acc: 0.9830 - val_loss: 0.0356 - val_acc: 0.9886\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 0.0388 - acc: 0.9943 - val_loss: 0.0270 - val_acc: 0.9886\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 0.0286 - acc: 0.9886 - val_loss: 0.0506 - val_acc: 0.9773\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.0845 - acc: 0.9659 - val_loss: 0.0227 - val_acc: 0.9943\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.0237 - acc: 0.9943 - val_loss: 0.0122 - val_acc: 0.9943\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.0124 - acc: 0.9943 - val_loss: 0.0071 - val_acc: 1.0000\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0271 - val_acc: 0.9943\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 0s 26ms/step - loss: 0.0267 - acc: 0.9943 - val_loss: 0.0178 - val_acc: 0.9886\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 0.0157 - acc: 0.9886 - val_loss: 0.0121 - val_acc: 0.9943\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 0.0119 - acc: 0.9943 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.0010 - val_acc: 1.0000\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 6.5050e-04 - val_acc: 1.0000\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 6.1431e-04 - acc: 1.0000 - val_loss: 3.5616e-04 - val_acc: 1.0000\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 3.5235e-04 - acc: 1.0000 - val_loss: 2.5644e-04 - val_acc: 1.0000\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 2.5546e-04 - acc: 1.0000 - val_loss: 1.8868e-04 - val_acc: 1.0000\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 1.8796e-04 - acc: 1.0000 - val_loss: 1.3761e-04 - val_acc: 1.0000\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 1.3736e-04 - acc: 1.0000 - val_loss: 8.6175e-05 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 8.5138e-05 - acc: 1.0000 - val_loss: 5.7705e-05 - val_acc: 1.0000\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 5.7192e-05 - acc: 1.0000 - val_loss: 4.4103e-05 - val_acc: 1.0000\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 4.3350e-05 - acc: 1.0000 - val_loss: 3.3067e-05 - val_acc: 1.0000\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 3.2830e-05 - acc: 1.0000 - val_loss: 2.5209e-05 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 2.5104e-05 - acc: 1.0000 - val_loss: 1.9097e-05 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 1.8986e-05 - acc: 1.0000 - val_loss: 1.4245e-05 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 1.4075e-05 - acc: 1.0000 - val_loss: 1.0845e-05 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 1.0796e-05 - acc: 1.0000 - val_loss: 8.2124e-06 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 8.1955e-06 - acc: 1.0000 - val_loss: 6.3280e-06 - val_acc: 1.0000\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 6.2545e-06 - acc: 1.0000 - val_loss: 4.2476e-06 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 4.2110e-06 - acc: 1.0000 - val_loss: 3.3017e-06 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 3.3054e-06 - acc: 1.0000 - val_loss: 2.5901e-06 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 2.5654e-06 - acc: 1.0000 - val_loss: 2.0584e-06 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 2.0611e-06 - acc: 1.0000 - val_loss: 1.6473e-06 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 1.6395e-06 - acc: 1.0000 - val_loss: 1.3242e-06 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 1.3174e-06 - acc: 1.0000 - val_loss: 1.0763e-06 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 1.0705e-06 - acc: 1.0000 - val_loss: 8.7070e-07 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 8.6562e-07 - acc: 1.0000 - val_loss: 6.7428e-07 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 6.6446e-07 - acc: 1.0000 - val_loss: 5.4051e-07 - val_acc: 1.0000\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 21ms/step - loss: 5.4084e-07 - acc: 1.0000 - val_loss: 4.6837e-07 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 4.6803e-07 - acc: 1.0000 - val_loss: 4.1080e-07 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 4.1046e-07 - acc: 1.0000 - val_loss: 3.5898e-07 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 3.6034e-07 - acc: 1.0000 - val_loss: 3.2512e-07 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 3.2478e-07 - acc: 1.0000 - val_loss: 2.9396e-07 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 2.9294e-07 - acc: 1.0000 - val_loss: 2.5705e-07 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.5637e-07 - acc: 1.0000 - val_loss: 2.3842e-07 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 2.3910e-07 - acc: 1.0000 - val_loss: 2.2352e-07 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 2.2352e-07 - acc: 1.0000 - val_loss: 2.1099e-07 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.1099e-07 - acc: 1.0000 - val_loss: 2.0049e-07 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 1.9913e-07 - acc: 1.0000 - val_loss: 1.8965e-07 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 1.8864e-07 - acc: 1.0000 - val_loss: 1.8118e-07 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 1.8152e-07 - acc: 1.0000 - val_loss: 1.7678e-07 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 1.7610e-07 - acc: 1.0000 - val_loss: 1.6764e-07 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 1.6798e-07 - acc: 1.0000 - val_loss: 1.6222e-07 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.6188e-07 - acc: 1.0000 - val_loss: 1.5612e-07 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.5714e-07 - acc: 1.0000 - val_loss: 1.5104e-07 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.5104e-07 - acc: 1.0000 - val_loss: 1.4800e-07 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 1.4867e-07 - acc: 1.0000 - val_loss: 1.4562e-07 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 1.4664e-07 - acc: 1.0000 - val_loss: 1.4292e-07 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 1.4393e-07 - acc: 1.0000 - val_loss: 1.4122e-07 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 1.4190e-07 - acc: 1.0000 - val_loss: 1.3817e-07 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 1.3885e-07 - acc: 1.0000 - val_loss: 1.3682e-07 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 1.3682e-07 - acc: 1.0000 - val_loss: 1.3479e-07 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 1.3479e-07 - acc: 1.0000 - val_loss: 1.3174e-07 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 1.3242e-07 - acc: 1.0000 - val_loss: 1.2903e-07 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 1.2937e-07 - acc: 1.0000 - val_loss: 1.2903e-07 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 1.2903e-07 - acc: 1.0000 - val_loss: 1.2869e-07 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 1.2903e-07 - acc: 1.0000 - val_loss: 1.2903e-07 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 0s 26ms/step - loss: 1.2903e-07 - acc: 1.0000 - val_loss: 1.2666e-07 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 1.2632e-07 - acc: 1.0000 - val_loss: 1.2564e-07 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 1.2564e-07 - acc: 1.0000 - val_loss: 1.2497e-07 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 1.2463e-07 - acc: 1.0000 - val_loss: 1.2395e-07 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 1.2395e-07 - acc: 1.0000 - val_loss: 1.2395e-07 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 1.2395e-07 - acc: 1.0000 - val_loss: 1.2293e-07 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 1.2293e-07 - acc: 1.0000 - val_loss: 1.2226e-07 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 1.2260e-07 - acc: 1.0000 - val_loss: 1.2226e-07 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 1.2226e-07 - acc: 1.0000 - val_loss: 1.2226e-07 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 1.2226e-07 - acc: 1.0000 - val_loss: 1.2192e-07 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 1.2192e-07 - acc: 1.0000 - val_loss: 1.2124e-07 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 1.2124e-07 - acc: 1.0000 - val_loss: 1.2124e-07 - val_acc: 1.0000\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "LSTM test loss    :  1.2124127124479855e-07\n",
      "LSTM test accuracy:  1.0\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 16, 100)           2800      \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 16, 200)           160800    \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 16, 200)           240800    \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 16, 28)            5628      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16, 28)            0         \n",
      "=================================================================\n",
      "Total params: 410,028\n",
      "Trainable params: 410,028\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM, Embedding, Bidirectional\n",
    "from keras import initializers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "word_embedding_size = 100\n",
    "vocab_size = len(vocab_dict) + 1\n",
    "\n",
    "hidden_units = 100\n",
    "batch_size = 2\n",
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "x_train = data_X\n",
    "y_train = data_YY\n",
    "\n",
    "x_test = data_X\n",
    "y_test = data_YY\n",
    "\n",
    "print(\"x_train.shape[1:] : \", x_train.shape[1:])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, word_embedding_size, input_length=x_train.shape[1]))\n",
    "# the model will take as input an integer matrix of size (batch, input_length).\n",
    "# the largest integer (i.e. word index) in the input should be\n",
    "# no larger than 999 (vocabulary size).\n",
    "# now model.output_shape == (None, 10, 64), where None is the batch dimension.\n",
    "\n",
    "model.add(Bidirectional(LSTM(hidden_units,\n",
    "               return_sequences=True,\n",
    "               input_shape=x_train.shape[1:])))\n",
    "\n",
    "model.add(Bidirectional(LSTM(hidden_units,\n",
    "               return_sequences=True)))\n",
    "               # input_shape=x_train.shape[1:]))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "rmsprop = RMSprop(lr=learning_rate)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=rmsprop,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "print('LSTM test loss    : ', scores[0])\n",
    "print('LSTM test accuracy: ', scores[1])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.7440005e-10 5.9073005e-13 4.3608742e-13 1.0000000e+00 3.5305674e-09\n",
      " 7.9776016e-14 1.2940489e-12 4.8662527e-09 1.0281051e-08 1.6972599e-14\n",
      " 1.5247533e-12 2.2429818e-11 2.3484835e-14 2.4010991e-12 1.2775712e-15\n",
      " 2.6728832e-12 9.9289670e-14 1.3178831e-13 5.2672363e-13 4.3059042e-16\n",
      " 1.2028978e-13 2.2367763e-13 4.3281405e-14 1.7780398e-12 5.2434449e-11\n",
      " 7.0216289e-12 1.8587108e-14 1.4519925e-14]\n",
      "3\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "3\n",
      "PRED:\n",
      "[[ 3  7  2  9  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  7 14  9  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  7  5  9  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [15  2  4 16  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 4 11  8  7  5  4  1  0  0  0  0  0  0  0  0  0]\n",
      " [ 4  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  8  6 12  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  8  6 12  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  8  6  4  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [13 18 13 19  2  5  4 20 21 22 23 10  3 24  1  0]\n",
      " [ 3 25 26  2  5 27  1  0  0  0  0  0  0  0  0  0]]\n",
      "REF :\n",
      "[[ 3  7  2  9  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  7 14  9  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  7  5  9  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [15  2  4 16  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 4 11  8  7  5  4  1  0  0  0  0  0  0  0  0  0]\n",
      " [ 4  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  8  6 12  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  8  6 12  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  8  6  4  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [13 18 13 19  2  5  4 20 21 22 23 10  3 24  1  0]\n",
      " [ 3 25 26  2  5 27  1  0  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "pred_x_test = model.predict(x_test)\n",
    "\n",
    "print(pred_x_test[0][0])\n",
    "print(np.argmax(pred_x_test[0][0]))\n",
    "\n",
    "print(y_test[0][0])\n",
    "print(np.argmax(y_test[0][0]))\n",
    "\n",
    "print(\"PRED:\")\n",
    "print(np.argmax(pred_x_test, axis=-1))\n",
    "\n",
    "print(\"REF :\")\n",
    "print(np.argmax(y_test, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
